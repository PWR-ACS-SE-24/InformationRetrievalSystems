export const response = {
  time_to_search: 2130,
  total: 10000,
  papers: [
    {
      doi: null,
      submitter: "Ashutosh Agarwal",
      abstract:
        "  Monocular Depth Estimation (MDE) aims to predict pixel-wise depth given a single RGB image. For both, the convolutional as well as the recent attention-based models, encoder-decoder-based architectures have been found to be useful due to the simultaneous requirement of global context and pixel-level resolution. Typically, a skip connection module is used to fuse the encoder and decoder features, which comprises of feature map concatenation followed by a convolution operation. Inspired by the demonstrated benefits of attention in a multitude of computer vision problems, we propose an attention-based fusion of encoder and decoder features. We pose MDE as a pixel query refinement problem, where coarsest-level encoder features are used to initialize pixel-level queries, which are then refined to higher resolutions by the proposed Skip Attention Module (SAM). We formulate the prediction problem as ordinal regression over the bin centers that discretize the continuous depth range and introduce a Bin Center Predictor (BCP) module that predicts bins at the coarsest level using pixel queries. Apart from the benefit of image adaptive depth binning, the proposed design helps learn improved depth embedding in initial pixel queries via direct supervision from the ground truth. Extensive experiments on the two canonical datasets, NYUV2 and KITTI, show that our architecture outperforms the state-of-the-art by 5.3 along with an improved generalization performance by 9.4 dataset. Code is available at https://github.com/ashutosh1807/PixelFormer.git. ",
      categories: ["cs.CV", "cs"],
      title:
        "Attention Attention Everywhere: Monocular Depth Prediction with Skip   Attention",
      arxiv_id: "2210.09071",
      authors: [
        "Agarwal Ashutosh",
        "Arora Chetan",
        "Ashutosh Agarwal",
        "Ashutosh Agarwal",
        "Ashutosh Agarwal",
        "Ashutosh Agarwal",
        "Ashutosh Agarwal",
        "Ashutosh Agarwal",
        "Ashutosh Agarwal",
        "Ashutosh Agarwal",
        "Ashutosh Agarwal",
        "Ashutosh Agarwal",
        "Ashutosh Agarwal",
        "Ashutosh Agarwal",
      ],
      journal_ref: null,
      comments:
        "Accepted at IEEE/CVF Winter Conference on Applications of Computer   Vision (WACV) 2023",
      update_date: "2022-10-18T00:00:00",
    },
    {
      doi: null,
      submitter: "Fenglin Liu",
      abstract:
        '  Recently, attention based models have been used extensively in many sequence-to-sequence learning systems. Especially for image captioning, the attention based models are expected to ground correct image regions with proper generated words. However, for each time step in the decoding process, the attention based models usually use the hidden state of the current input to attend to the image regions. Under this setting, these attention models have a "deviated focus" problem that they calculate the attention weights based on previous words instead of the one to be generated, impairing the performance of both grounding and captioning. In this paper, we propose the Prophet Attention, similar to the form of self-supervision. In the training stage, this module utilizes the future information to calculate the "ideal" attention weights towards image regions. These calculated "ideal" weights are further used to regularize the "deviated" attention. In this manner, image regions are grounded with the correct words. The proposed Prophet Attention can be easily incorporated into existing image captioning models to improve their performance of both grounding and captioning. The experiments on the Flickr30k Entities and the MSCOCO datasets show that the proposed Prophet Attention consistently outperforms baselines in both automatic metrics and human evaluations. It is worth noticing that we set new state-of-the-arts on the two benchmark datasets and achieve the 1st place on the leaderboard of the online MSCOCO benchmark in terms of the default ranking score, i.e., CIDEr-c40. ',
      categories: ["cs.CV", "cs", "cs.CL"],
      title:
        "Prophet Attention: Predicting Attention with Future Attention for Image   Captioning",
      arxiv_id: "2210.10914",
      authors: [
        "Wu Xian",
        "Sun Xu",
        "Fenglin Liu",
        "Zou Yuexian",
        "Liu Fenglin",
        "Ren Xuancheng",
        "Fan Wei",
      ],
      journal_ref: null,
      comments: "Accepted by NeurIPS 2020",
      update_date: "2023-04-12T00:00:00",
    },
    {
      doi: null,
      submitter: "Zhuoran Shen",
      abstract:
        "  Dot-product attention has wide applications in computer vision and natural language processing. However, its memory and computational costs grow quadratically with the input size. Such growth prohibits its application on high-resolution inputs. To remedy this drawback, this paper proposes a novel efficient attention mechanism equivalent to dot-product attention but with substantially less memory and computational costs. Its resource efficiency allows more widespread and flexible integration of attention modules into a network, which leads to better accuracies. Empirical evaluations demonstrated the effectiveness of its advantages. Efficient attention modules brought significant performance boosts to object detectors and instance segmenters on MS-COCO 2017. Further, the resource efficiency democratizes attention to complex models, where high costs prohibit the use of dot-product attention. As an exemplar, a model with efficient attention achieved state-of-the-art accuracies for stereo depth estimation on the Scene Flow dataset. Code is available at https://github.com/cmsflash/efficient-attention. ",
      categories: ["cs.CV", "cs.LG", "cs", "cs.AI"],
      title: "Efficient Attention: Attention with Linear Complexities",
      arxiv_id: "1812.01243",
      authors: [
        "Shen Zhuoran",
        "Zhuoran Shen",
        "Yi Shuai",
        "Zhang Mingyuan",
        "Li Hongsheng",
        "Zhao Haiyu",
      ],
      journal_ref: null,
      comments: "To appear at WACV 2021",
      update_date: "2024-01-22T00:00:00",
    },
    {
      doi: null,
      submitter: "Lun Huang",
      abstract:
        "  Attention mechanisms are widely used in current encoder/decoder frameworks of image captioning, where a weighted average on encoded vectors is generated at each time step to guide the caption decoding process. However, the decoder has little idea of whether or how well the attended vector and the given attention query are related, which could make the decoder give misled results. In this paper, we propose an Attention on Attention (AoA) module, which extends the conventional attention mechanisms to determine the relevance between attention results and queries. AoA first generates an information vector and an attention gate using the attention result and the current context, then adds another attention by applying element-wise multiplication to them and finally obtains the attended information, the expected useful knowledge. We apply AoA to both the encoder and the decoder of our image captioning model, which we name as AoA Network (AoANet). Experiments show that AoANet outperforms all previously published methods and achieves a new state-of-the-art performance of 129.8 CIDEr-D score on MS COCO Karpathy offline test split and 129.6 CIDEr-D (C40) score on the official online testing server. Code is available at https://github.com/husthuaan/AoANet. ",
      categories: ["cs.CV", "cs"],
      title: "Attention on Attention for Image Captioning",
      arxiv_id: "1908.06954",
      authors: [
        "Wang Wenmin",
        "Lun Huang",
        "Chen Jie",
        "Wei Xiao-Yong",
        "Huang Lun",
      ],
      journal_ref: null,
      comments: "Accepted to ICCV 2019 (Oral)",
      update_date: "2019-08-22T00:00:00",
    },
    {
      doi: null,
      submitter: "Andy Yang",
      abstract:
        "  We study conditions under which transformers using soft attention can simulate hard attention, that is, effectively focus all attention on a subset of positions. First, we examine several variants of linear temporal logic, whose formulas have been previously been shown to be computable using hard attention transformers. We demonstrate how soft attention transformers can compute formulas of these logics using unbounded positional embeddings or temperature scaling. Second, we demonstrate how temperature scaling allows softmax transformers to simulate a large subclass of average-hard attention transformers, those that have what we call the uniform-tieless property. ",
      categories: ["cs.FL", "cs.LG", "cs", "cs.CL"],
      title: "Simulating Hard Attention Using Soft Attention",
      arxiv_id: "2412.09925",
      authors: [
        "Yang Andy",
        "Andy Yang",
        "Angluin Dana",
        "Strobl Lena",
        "Chiang David",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2024-12-16T00:00:00",
    },
    {
      doi: null,
      submitter: "Yuli Liu",
      abstract:
        "  Transformer-based approaches have demonstrated remarkable success in various sequence-based tasks. However, traditional self-attention models may not sufficiently capture the intricate dependencies within items in sequential recommendation scenarios. This is due to the lack of explicit emphasis on attention weights, which play a critical role in allocating attention and understanding item-to-item correlations. To better exploit the potential of attention weights and improve the capability of sequential recommendation in learning high-order dependencies, we propose a novel sequential recommendation (SR) approach called attention weight refinement (AWRSR). AWRSR enhances the effectiveness of self-attention by additionally paying attention to attention weights, allowing for more refined attention distributions of correlations among items. We conduct comprehensive experiments on multiple real-world datasets, demonstrating that our approach consistently outperforms state-of-the-art SR models. Moreover, we provide a thorough analysis of AWRSR's effectiveness in capturing higher-level dependencies. These findings suggest that AWRSR offers a promising new direction for enhancing the performance of self-attention architecture in SR tasks, with potential applications in other sequence-based problems as well. ",
      categories: ["cs.IR", "cs"],
      title: "Pay Attention to Attention for Sequential Recommendation",
      arxiv_id: "2410.21048",
      authors: ["Liu Yuli", "Liu Xiaojing", "Liu Min", "Yuli Liu"],
      journal_ref: null,
      comments: "Accepted at RecSys 2024",
      update_date: "2024-10-29T00:00:00",
    },
    {
      doi: null,
      submitter: "Lev Utkin",
      abstract:
        "  New models of random forests jointly using the attention and self-attention mechanisms are proposed for solving the regression problem. The models can be regarded as extensions of the attention-based random forest whose idea stems from applying a combination of the Nadaraya-Watson kernel regression and the Huber's contamination model to random forests. The self-attention aims to capture dependencies of the tree predictions and to remove noise or anomalous predictions in the random forest. The self-attention module is trained jointly with the attention module for computing weights. It is shown that the training process of attention weights is reduced to solving a single quadratic or linear optimization problem. Three modifications of the general approach are proposed and compared. A specific multi-head self-attention for the random forest is also considered. Heads of the self-attention are obtained by changing its tuning parameters including the kernel parameters and the contamination parameter of models. Numerical experiments with various datasets illustrate the proposed models and show that the supplement of the self-attention improves the model performance for many datasets. ",
      categories: ["stat.ML", "cs.LG", "cs", "stat"],
      title: "Attention and Self-Attention in Random Forests",
      arxiv_id: "2207.04293",
      authors: ["Utkin Lev V.", "Lev Utkin", "Konstantinov Andrei V."],
      journal_ref: null,
      comments: "arXiv admin note: text overlap with arXiv:2201.02880",
      update_date: "2022-07-12T00:00:00",
    },
    {
      doi: null,
      submitter: "Claudius Gros",
      abstract:
        "  Attention regulates information transfer between tokens. For this, query and key vectors are compared, typically in terms of a scalar product, ùêê^Tùêä, together with a subsequent softmax normalization. In geometric terms, the standard dot-product attention (DPA) leads to large/small attention weights for parallel/antiparallel queries and keys. Here we study expressive attention (EA), which is based on (ùêê^Tùêä)^2, the squared dot product. In this case, attention is enhanced when query and key are either parallel or antiparallel, and suppressed for orthogonal configurations. EA can be introduced into any attention-based code without additional compute costs or memory requirements. For a series of autoregressive prediction tasks, we find that expressive attention performs at least as well as vanilla DPA. Increasing task complexity, EA is observed to outperform DPA with increasing margins, which also holds for multi-task settings. For a given model size, EA manages to achieve 100 accessible to DPA. Our results show that it is possible to reorganize the geometry of the matching condition in the space of attention heads without loss of performance. ",
      categories: ["cs.LG", "cs", "cs.AI"],
      title: "Reorganizing attention-space geometry with expressive attention",
      arxiv_id: "2407.18601",
      authors: ["Claudius Gros", "Gros Claudius"],
      journal_ref: null,
      comments: "",
      update_date: "2025-01-09T00:00:00",
    },
    {
      doi: null,
      submitter: "Anirudh Vemula",
      abstract:
        "  Robots that navigate through human crowds need to be able to plan safe, efficient, and human predictable trajectories. This is a particularly challenging problem as it requires the robot to predict future human trajectories within a crowd where everyone implicitly cooperates with each other to avoid collisions. Previous approaches to human trajectory prediction have modeled the interactions between humans as a function of proximity. However, that is not necessarily true as some people in our immediate vicinity moving in the same direction might not be as important as other people that are further away, but that might collide with us in the future. In this work, we propose Social Attention, a novel trajectory prediction model that captures the relative importance of each person when navigating in the crowd, irrespective of their proximity. We demonstrate the performance of our method against a state-of-the-art approach on two publicly available crowd datasets and analyze the trained attention model to gain a better understanding of which surrounding agents humans attend to, when navigating in a crowd. ",
      categories: ["cs.LG", "cs", "cs.RO"],
      title: "Social Attention: Modeling Attention in Human Crowds",
      arxiv_id: "1710.04689",
      authors: [
        "Anirudh Vemula",
        "Vemula Anirudh",
        "Muelling Katharina",
        "Oh Jean",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2018-10-31T00:00:00",
    },
    {
      doi: null,
      submitter: "Daniel Bolya",
      abstract:
        "  While transformers have begun to dominate many tasks in vision, applying them to large images is still computationally difficult. A large reason for this is that self-attention scales quadratically with the number of tokens, which in turn, scales quadratically with the image size. On larger images (e.g., 1080p), over 60 and applying attention matrices. We take a step toward solving this issue by introducing Hydra Attention, an extremely efficient attention operation for Vision Transformers (ViTs). Paradoxically, this efficiency comes from taking multi-head attention to its extreme: by using as many attention heads as there are features, Hydra Attention is computationally linear in both tokens and features with no hidden constants, making it significantly faster than standard self-attention in an off-the-shelf ViT-B/16 by a factor of the token count. Moreover, Hydra Attention retains high accuracy on ImageNet and, in some cases, actually improves it. ",
      categories: ["cs.CV", "cs"],
      title: "Hydra Attention: Efficient Attention with Many Heads",
      arxiv_id: "2209.07484",
      authors: [
        "Daniel Bolya",
        "Fu Cheng-Yang",
        "Zhang Peizhao",
        "Hoffman Judy",
        "Dai Xiaoliang",
        "Bolya Daniel",
      ],
      journal_ref: null,
      comments: "Accepted CADL 2022 (ECCV Workshop)",
      update_date: "2022-09-16T00:00:00",
    },
    {
      doi: null,
      submitter: "Sivakorn Sanguanmoo",
      abstract:
        "  We develop a unified analysis of how information captures attention. A decision maker (DM) faces a dynamic information structure and decides when to stop paying attention. We characterize the convexx2013order frontier and extreme points of feasible stopping times, as well as dynamic information structures which implement them. This delivers the form of optimal attentional capture as a function of the designer and DM's relative time preferences. Intertemporal commitment is unnecessary: sequentially optimal information structures always exist by inducing stochastic interim beliefs. We further analyze optimal attention capture under non instrumental value of information. Our results speak directly to the attention economy. ",
      categories: ["econ.TH", "econ"],
      title: "Attention Capture",
      arxiv_id: "2209.05570",
      authors: ["Sivakorn Sanguanmoo", "Koh Andrew", "Sanguanmoo Sivakorn"],
      journal_ref: null,
      comments: "79 pages",
      update_date: "2024-09-25T00:00:00",
    },
    {
      doi: null,
      submitter: "Yiran Zhong",
      abstract:
        "  Attention has been proved to be an efficient mechanism to capture long-range dependencies. However, so far it has not been deployed in invertible networks. This is due to the fact that in order to make a network invertible, every component within the network needs to be a bijective transformation, but a normal attention block is not. In this paper, we propose invertible attention that can be plugged into existing invertible models. We mathematically and experimentally prove that the invertibility of an attention model can be achieved by carefully constraining its Lipschitz constant. We validate the invertibility of our invertible attention on image reconstruction task with 3 popular datasets: CIFAR-10, SVHN, and CelebA. We also show that our invertible attention achieves similar performance in comparison with normal non-invertible attention on dense prediction tasks. The code is available at https://github.com/Schwartz-Zha/InvertibleAttention ",
      categories: ["cs.CV", "cs"],
      title: "Invertible Attention",
      arxiv_id: "2106.09003",
      authors: [
        "Zha Jiajun",
        "Zhong Yiran",
        "Zheng Liang",
        "Zhang Jing",
        "Yiran Zhong",
        "Hartley Richard",
      ],
      journal_ref: null,
      comments:
        "19 pages. The code is available at   https://github.com/Schwartz-Zha/InvertibleAttention",
      update_date: "2021-06-29T00:00:00",
    },
    {
      doi: null,
      submitter: "Tan Nguyen",
      abstract:
        "  Pairwise dot-product self-attention is key to the success of transformers that achieve state-of-the-art performance across a variety of applications in language and vision. This dot-product self-attention computes attention weights among the input tokens using Euclidean distance, which makes the model prone to representation collapse and vulnerable to contaminated samples. In this paper, we propose using a Mahalanobis distance metric for computing the attention weights to stretch the underlying feature space in directions of high contextual relevance. In particular, we define a hyper-ellipsoidal neighborhood around each query to increase the attention weights of the tokens lying in the contextually important directions. We term this novel class of attention Elliptical Attention. Our Elliptical Attention provides two benefits: 1) reducing representation collapse and 2) enhancing the model's robustness as Elliptical Attention pays more attention to contextually relevant information rather than focusing on some small subset of informative features. We empirically demonstrate the advantages of Elliptical Attention over the baseline dot-product attention and state-of-the-art attention methods on various practical tasks, including object classification, image segmentation, and language modeling across different data modalities. ",
      categories: ["stat.ML", "cs.LG", "cs.CV", "cs.CL", "cs", "cs.AI", "stat"],
      title: "Elliptical Attention",
      arxiv_id: "2406.13770",
      authors: [
        "Abdullaev Laziz U.",
        "Nguyen Tan M.",
        "Nielsen Stefan K.",
        "Tan Nguyen",
        "Teo Rachel S. Y.",
      ],
      journal_ref: null,
      comments:
        "10 pages in the main text. Published at NeurIPS 2024. The code is   available at https://github.com/stefvk/Elliptical-Attention",
      update_date: "2024-11-04T00:00:00",
    },
    {
      doi: null,
      submitter: "Xinwei Ma",
      abstract:
        "  We introduce an Attention Overload Model that captures the idea that alternatives compete for the decision maker's attention, and hence the attention that each alternative receives decreases as the choice problem becomes larger. Using this nonparametric restriction on the random attention formation, we show that a fruitful revealed preference theory can be developed and provide testable implications on the observed choice behavior that can be used to (point or partially) identify the decision maker's preference and attention frequency. We then enhance our attention overload model to accommodate heterogeneous preferences. Due to the nonparametric nature of our identifying assumption, we must discipline the amount of heterogeneity in the choice model: we propose the idea of List-based Attention Overload, where alternatives are presented to the decision makers as a list that correlates with both heterogeneous preferences and random attention. We show that preference and attention frequencies are (point or partially) identifiable under nonparametric assumptions on the list and attention formation mechanisms, even when the true underlying list is unknown to the researcher. Building on our identification results, for both preference and attention frequencies, we develop econometric methods for estimation and inference that are valid in settings with a large number of alternatives and choice problems, a distinctive feature of the economic environment we consider. We provide a software package in R implementing our empirical methods, and illustrate them in a simulation study. ",
      categories: ["econ.TH", "econ.EM", "econ"],
      title: "Attention Overload",
      arxiv_id: "2110.10650",
      authors: [
        "Ma Xinwei",
        "Cheung Paul",
        "Cattaneo Matias D.",
        "Masatlioglu Yusufcan",
        "Xinwei Ma",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2024-09-17T00:00:00",
    },
    {
      doi: null,
      submitter: "Matthew Spellings",
      abstract:
        "  Neural networks using transformer-based architectures have recently demonstrated great power and flexibility in modeling sequences of many types. One of the core components of transformer networks is the attention layer, which allows contextual information to be exchanged among sequence elements. While many of the prevalent network structures thus far have utilized full attention ‚Äì which operates on all pairs of sequence elements ‚Äì the quadratic scaling of this attention mechanism significantly constrains the size of models that can be trained. In this work, we present an attention model that has only linear requirements in memory and computation time. We show that, despite the simpler attention model, networks using this attention mechanism can attain comparable performance to full attention networks on language modeling tasks. ",
      categories: ["stat.ML", "cs.LG", "cs", "stat"],
      title: "Agglomerative Attention",
      arxiv_id: "1907.06607",
      authors: ["Spellings Matthew", "Matthew Spellings"],
      journal_ref: null,
      comments: "7 pages, 3 figures",
      update_date: "2019-07-16T00:00:00",
    },
    {
      doi: null,
      submitter: "Yang Li",
      abstract:
        "  Existing attention mechanisms are trained to attend to individual items in a collection (the memory) with a predefined, fixed granularity, e.g., a word token or an image grid. We propose area attention: a way to attend to areas in the memory, where each area contains a group of items that are structurally adjacent, e.g., spatially for a 2D memory such as images, or temporally for a 1D memory such as natural language sentences. Importantly, the shape and the size of an area are dynamically determined via learning, which enables a model to attend to information with varying granularity. Area attention can easily work with existing model architectures such as multi-head attention for simultaneously attending to multiple areas in the memory. We evaluate area attention on two tasks: neural machine translation (both character and token-level) and image captioning, and improve upon strong (state-of-the-art) baselines in all the cases. These improvements are obtainable with a basic form of area attention that is parameter free. ",
      categories: ["stat.ML", "cs.LG", "cs", "cs.CL", "stat", "cs.AI"],
      title: "Area Attention",
      arxiv_id: "1810.10126",
      authors: ["Bengio Samy", "Yang Li", "Li Yang", "Si Si", "Kaiser Lukasz"],
      journal_ref: null,
      comments:
        "@InProceedingspmlr-v97-li19e, title = Area Attention, author =   Li, Yang and Kaiser, Lukasz and Bengio, Samy and Si, Si, booktitle =   Proceedings of the 36th International Conference on Machine Learning, pages   = 3846‚Äì3855, year = 2019, volume = 97, series = Proceedings of   Machine Learning Research, publisher = PMLR ",
      update_date: "2020-05-11T00:00:00",
    },
    {
      doi: null,
      submitter: "Chiwun Yang",
      abstract:
        "  Sparse Attention is a technique that approximates standard attention computation with sub-quadratic complexity. This is achieved by selectively ignoring smaller entries in the attention matrix during the softmax function computation. Variations of this technique, such as pruning KV cache, sparsity-based fast attention, and Sparse Transformer, have been extensively utilized for efficient Large Language Models (LLMs) deployment. Despite its widespread use, a theoretical understanding of the conditions under which sparse attention performs on par with traditional attention remains elusive. This work aims to bridge this gap by examining the inherent sparsity of standard attention processes. Our theoretical framework reveals several brand-new key insights:   ‚àô Attention is n^C-sparse, implying that considering only the largest Œ©(n^C) entries out of all n entries is sufficient for sparse attention to approximate the exact attention matrix with decreasing loss. Here, n represents the input length and C ‚àà (0, 1) is a constant.   ‚àô Stable o(log(n))-sparse attention, which approximates attention computation with log(n) or fewer entries, may not be feasible since the error will persist at a minimum of O(1).   ‚àô An adaptive strategy (Œ±¬∑ n^C, Œ±‚àà‚Ñù) for the window size of efficient attention methods rather than a fixed one is guaranteed to perform more accurately and efficiently in a task for inference on flexible context lengths. ",
      categories: ["cs.CL", "cs.LG", "cs", "cs.AI"],
      title:
        "How Sparse Attention Approximates Exact Attention? Your Attention is   Naturally n^C-Sparse",
      arxiv_id: "2404.02690",
      authors: [
        "Xiong Jing",
        "Chiwun Yang",
        "Yang Chiwun",
        "Deng Yichuan",
        "Song Zhao",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2025-02-13T00:00:00",
    },
    {
      doi: null,
      submitter: "Aniruddha Nrusimha",
      abstract:
        "  To help address the growing demand for ever-longer sequence lengths in transformer models, Liu et al. recently proposed Ring Attention, an exact attention algorithm capable of overcoming per-device memory bottle- necks by distributing self-attention across multiple devices. In this paper, we study the performance characteristics of Ring Attention in the important special case of causal transformer models, and identify a key workload imbal- ance due to triangular structure of causal attention computations. We propose a simple extension to Ring Attention, which we call Striped Attention to fix this imbalance. Instead of devices having contiguous subsequences, each device has a subset of tokens distributed uniformly throughout the sequence, which we demonstrate leads to more even workloads. In experiments running Striped Attention on A100 GPUs and TPUv4s, we are able to achieve up to 1.45x end-to-end throughput improvements over the original Ring Attention algorithm on causal transformer training at a sequence length of 256k. Furthermore, on 16 TPUv4 chips, we were able to achieve 1.65x speedups at sequence lengths of 786k. We release the code for our experiments as open source ",
      categories: ["cs.LG", "cs", "cs.CL"],
      title: "Striped Attention: Faster Ring Attention for Causal Transformers",
      arxiv_id: "2311.09431",
      authors: [
        "Nrusimha Aniruddha",
        "Qian Kevin",
        "Ankner Zachary",
        "Song Zhiye",
        "Aniruddha Nrusimha",
        "Brandon William",
        "Jin Tian",
        "Ragan-Kelley Jonathan",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2023-11-17T00:00:00",
    },
    {
      doi: null,
      submitter: "Haoyu Chen",
      abstract:
        "  Convolutional neural networks have allowed remarkable advances in single image super-resolution (SISR) over the last decade. Among recent advances in SISR, attention mechanisms are crucial for high-performance SR models. However, the attention mechanism remains unclear on why and how it works in SISR. In this work, we attempt to quantify and visualize attention mechanisms in SISR and show that not all attention modules are equally beneficial. We then propose attention in attention network (A^2N) for more efficient and accurate SISR. Specifically, A^2N consists of a non-attention branch and a coupling attention branch. A dynamic attention module is proposed to generate weights for these two branches to suppress unwanted attention adjustments dynamically, where the weights change adaptively according to the input features. This allows attention modules to specialize to beneficial examples without otherwise penalties and thus greatly improve the capacity of the attention network with few parameters overhead. Experimental results demonstrate that our final model A^2N could achieve superior trade-off performances comparing with state-of-the-art networks of similar sizes. Codes are available at https://github.com/haoyuc/A2N. ",
      categories: ["cs.CV", "cs"],
      title: "Attention in Attention Network for Image Super-Resolution",
      arxiv_id: "2104.09497",
      authors: ["Haoyu Chen", "Zhang Zhi", "Chen Haoyu", "Gu Jinjin"],
      journal_ref: null,
      comments:
        "11 pages, 10 figures. Codes are available at   https://github.com/haoyuc/A2N",
      update_date: "2021-11-09T00:00:00",
    },
    {
      doi: null,
      submitter: "Heejun Lee",
      abstract:
        "  The transformer architecture has driven breakthroughs in recent years on tasks which require modeling pairwise relationships between sequential elements, as is the case in natural language understanding. However, long seqeuences pose a problem due to the quadratic complexity of the attention operation. Previous research has aimed to lower the complexity by sparsifying or linearly approximating the attention matrix. Yet, these approaches cannot straightforwardly distill knowledge from a teacher's attention matrix and often require complete retraining from scratch. Furthermore, previous sparse and linear approaches lose interpretability if they cannot produce full attention matrices. To address these challenges, we propose SEA: Sparse linear attention with an Estimated Attention mask. SEA estimates the attention matrix with linear complexity via kernel-based linear attention, then subsequently creates a sparse attention matrix with a top-k selection to perform a sparse attention operation. For language modeling tasks (Wikitext2), previous linear and sparse attention methods show roughly two-fold worse perplexity scores over the quadratic OPT-1.3B baseline, while SEA achieves better perplexity than OPT-1.3B, using roughly half the memory of OPT-1.3B, providing interpretable attention matrix. We believe that our work will have a large practical impact, as it opens the possibility of running large transformers on resource-limited devices with less memory. ",
      categories: ["cs.LG", "cs", "cs.CL"],
      title: "SEA: Sparse Linear Attention with Estimated Attention Mask",
      arxiv_id: "2310.01777",
      authors: [
        "Hwang Sung Ju",
        "Heejun Lee",
        "Lee Heejun",
        "Kim Jina",
        "Willette Jeffrey",
      ],
      journal_ref: null,
      comments: "9 main pages",
      update_date: "2024-03-26T00:00:00",
    },
    {
      doi: null,
      submitter: "Markus Krimmel",
      abstract:
        "  Object-centric scene decompositions are important representations for downstream tasks in fields such as computer vision and robotics. The recently proposed Slot Attention module, already leveraged by several derivative works for image segmentation and object tracking in videos, is a deep learning component which performs unsupervised object-centric scene decomposition on input images. It is based on an attention architecture, in which latent slot vectors, which hold compressed information on objects, attend to localized perceptual features from the input image. In this paper, we demonstrate that design decisions on normalizing the aggregated values in the attention architecture have considerable impact on the capabilities of Slot Attention to generalize to a higher number of slots and objects as seen during training. We propose and investigate alternatives to the original normalization scheme which increase the generalization capabilities of Slot Attention to varying slot and object counts, resulting in performance gains on the task of unsupervised image segmentation. The newly proposed normalizations represent minimal and easy to implement modifications of the usual Slot Attention module, changing the value aggregation mechanism from a weighted mean operation to a scaled weighted sum operation. ",
      categories: ["cs.CV", "cs"],
      title:
        "Attention Normalization Impacts Cardinality Generalization in Slot   Attention",
      arxiv_id: "2407.04170",
      authors: [
        "Krimmel Markus",
        "Stueckler Joerg",
        "Markus Krimmel",
        "Achterhold Jan",
      ],
      journal_ref: null,
      comments: "30 pages",
      update_date: "2024-11-12T00:00:00",
    },
    {
      doi: "10.1109/WACV45572.2020.9093515",
      submitter: '\\"Omer S\\"umer',
      abstract:
        "  This paper addresses the problem of understanding joint attention in third-person social scene videos. Joint attention is the shared gaze behaviour of two or more individuals on an object or an area of interest and has a wide range of applications such as human-computer interaction, educational assessment, treatment of patients with attention disorders, and many more. Our method, Attention Flow, learns joint attention in an end-to-end fashion by using saliency-augmented attention maps and two novel convolutional attention mechanisms that determine to select relevant features and improve joint attention localization. We compare the effect of saliency maps and attention mechanisms and report quantitative and qualitative results on the detection and localization of joint attention in the VideoCoAtt dataset, which contains complex social scenes. ",
      categories: ["cs.CV", "cs"],
      title: "Attention Flow: End-to-End Joint Attention Estimation",
      arxiv_id: "2001.03960",
      authors: [
        "S√ºmer √ñmer",
        '\\"Omer S\\"umer',
        "Kasneci Enkelejda",
        "Trautwein Ulrich",
        "Gerjets Peter",
      ],
      journal_ref: null,
      comments: "Paper accepted in WACV 2020",
      update_date: "2021-01-13T00:00:00",
    },
    {
      doi: null,
      submitter: "Yu Yan",
      abstract:
        "  Transformer model with multi-head attention requires caching intermediate results for efficient inference in generation tasks. However, cache brings new memory-related costs and prevents leveraging larger batch size for faster speed. We propose memory-efficient lossless attention (called EL-attention) to address this issue. It avoids heavy operations for building multi-head keys and values, cache for them is not needed. EL-attention constructs an ensemble of attention results by expanding query while keeping key and value shared. It produces the same result as multi-head attention with less GPU memory and faster inference speed. We conduct extensive experiments on Transformer, BART, and GPT-2 for summarization and question generation tasks. The results show EL-attention speeds up existing models by 1.6x to 5.3x without accuracy loss. ",
      categories: ["cs.LG", "cs", "cs.CL"],
      title: "EL-Attention: Memory Efficient Lossless Attention for Generation",
      arxiv_id: "2105.04779",
      authors: [
        "Duan Nan",
        "Bhendawade Nikhil",
        "Zhang Ruofei",
        "Yan Yu",
        "Yu Yan",
        "Qi Weizhen",
        "Gong Yeyun",
        "Chen Jiusheng",
      ],
      journal_ref: null,
      comments: "ICML 2021. Version 2: add pseudocode",
      update_date: "2021-06-15T00:00:00",
    },
    {
      doi: null,
      submitter: "Jayaraman J. Thiagarajan",
      abstract:
        "  Machine learning models that can exploit the inherent structure in data have gained prominence. In particular, there is a surge in deep learning solutions for graph-structured data, due to its wide-spread applicability in several fields. Graph attention networks (GAT), a recent addition to the broad class of feature learning models in graphs, utilizes the attention mechanism to efficiently learn continuous vector representations for semi-supervised learning problems. In this paper, we perform a detailed analysis of GAT models, and present interesting insights into their behavior. In particular, we show that the models are vulnerable to heterogeneous rogue nodes and hence propose novel regularization strategies to improve the robustness of GAT models. Using benchmark datasets, we demonstrate performance improvements on semi-supervised learning, using the proposed robust variant of GAT. ",
      categories: ["stat.ML", "cs.LG", "cs", "stat"],
      title: "A Regularized Attention Mechanism for Graph Attention Networks",
      arxiv_id: "1811.00181",
      authors: [
        "Spanias Andreas",
        "Thiagarajan Jayaraman J.",
        "Jayaraman J. Thiagarajan",
        "Shanthamallu Uday Shankar",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2020-02-12T00:00:00",
    },
    {
      doi: null,
      submitter: "Shi  Chen",
      abstract:
        "  Visual attention has shown usefulness in image captioning, with the goal of enabling a caption model to selectively focus on regions of interest. Existing models typically rely on top-down language information and learn attention implicitly by optimizing the captioning objectives. While somewhat effective, the learned top-down attention can fail to focus on correct regions of interest without direct supervision of attention. Inspired by the human visual system which is driven by not only the task-specific top-down signals but also the visual stimuli, we in this work propose to use both types of attention for image captioning. In particular, we highlight the complementary nature of the two types of attention and develop a model (Boosted Attention) to integrate them for image captioning. We validate the proposed approach with state-of-the-art performance across various evaluation metrics. ",
      categories: ["cs.CV", "cs"],
      title:
        "Boosted Attention: Leveraging Human Attention for Image Captioning",
      arxiv_id: "1904.00767",
      authors: ["Shi  Chen", "Zhao Qi", "Chen Shi"],
      journal_ref: null,
      comments: "Published in ECCV 2018",
      update_date: "2019-04-02T00:00:00",
    },
    {
      doi: "10.18653/v1/P17-1055",
      submitter: "Yiming Cui",
      abstract:
        '  Cloze-style queries are representative problems in reading comprehension. Over the past few months, we have seen much progress that utilizing neural network approach to solve Cloze-style questions. In this paper, we present a novel model called attention-over-attention reader for the Cloze-style reading comprehension task. Our model aims to place another attention mechanism over the document-level attention, and induces "attended attention" for final predictions. Unlike the previous works, our neural network model requires less pre-defined hyper-parameters and uses an elegant architecture for modeling. Experimental results show that the proposed attention-over-attention model significantly outperforms various state-of-the-art systems by a large margin in public datasets, such as CNN and Children\'s Book Test datasets. ',
      categories: ["cs.NE", "cs", "cs.CL"],
      title:
        "Attention-over-Attention Neural Networks for Reading Comprehension",
      arxiv_id: "1607.04423",
      authors: [
        "Hu Guoping",
        "Wei Si",
        "Chen Zhipeng",
        "Cui Yiming",
        "Wang Shijin",
        "Liu Ting",
        "Yiming Cui",
      ],
      journal_ref: null,
      comments:
        "8+2 pages. accepted as a conference paper at ACL2017 (long paper)",
      update_date: "2019-09-02T00:00:00",
    },
    {
      doi: null,
      submitter: "Shenyuan Gao",
      abstract:
        "  Transformer trackers have achieved impressive advancements recently, where the attention mechanism plays an important role. However, the independent correlation computation in the attention mechanism could result in noisy and ambiguous attention weights, which inhibits further performance improvement. To address this issue, we propose an attention in attention (AiA) module, which enhances appropriate correlations and suppresses erroneous ones by seeking consensus among all correlation vectors. Our AiA module can be readily applied to both self-attention blocks and cross-attention blocks to facilitate feature aggregation and information propagation for visual tracking. Moreover, we propose a streamlined Transformer tracking framework, dubbed AiATrack, by introducing efficient feature reuse and target-background embeddings to make full use of temporal references. Experiments show that our tracker achieves state-of-the-art performance on six tracking benchmarks while running at a real-time speed. ",
      categories: ["cs.CV", "cs"],
      title: "AiATrack: Attention in Attention for Transformer Visual Tracking",
      arxiv_id: "2207.09603",
      authors: [
        "Wang Xinggang",
        "Shenyuan Gao",
        "Zhou Chunluan",
        "Gao Shenyuan",
        "Ma Chao",
        "Yuan Junsong",
      ],
      journal_ref: null,
      comments:
        "Accepted by ECCV 2022. Code and models are publicly available at   https://github.com/Little-Podi/AiATrack",
      update_date: "2022-07-25T00:00:00",
    },
    {
      doi: null,
      submitter: "Yixing Xu",
      abstract:
        "  Transformer-based LLMs have achieved exceptional performance across a wide range of NLP tasks. However, the standard self-attention mechanism suffers from quadratic time complexity and linearly increased cache size. Sliding window attention (SWA) solves this problem by restricting the attention range to a fixed-size local context window. Nevertheless, SWA employs a uniform window size for each head in each layer, making it inefficient in capturing context of varying scales. To mitigate this limitation, we propose Multi-Scale Window Attention (MSWA) which applies diverse window sizes across heads and layers in the Transformer. It not only allows for different window sizes among heads within the same layer but also progressively increases window size allocation from shallow to deep layers, thus enabling the model to capture contextual information with different lengths and distances. Experimental results on language modeling and common-sense reasoning tasks substantiate that MSWA outperforms traditional local attention in both effectiveness and efficiency. ",
      categories: ["cs.AI", "cs", "cs.CL"],
      title: "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
      arxiv_id: "2501.01039",
      authors: [
        "Barsoum Emad",
        "Yixing Xu",
        "Tian Lu",
        "Nag Shivank",
        "Xu Yixing",
        "Li Dong",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2025-01-03T00:00:00",
    },
    {
      doi: "10.1109/FG52635.2021.9666970",
      submitter: "Xiaotian Li",
      abstract:
        '  Visual attention has been extensively studied for learning fine-grained features in both facial expression recognition (FER) and Action Unit (AU) detection. A broad range of previous research has explored how to use attention modules to localize detailed facial parts (e,g. facial action units), learn discriminative features, and learn inter-class correlation. However, few related works pay attention to the robustness of the attention module itself. Through experiments, we found neural attention maps initialized with different feature maps yield diverse representations when learning to attend the identical Region of Interest (ROI). In other words, similar to general feature learning, the representational quality of attention maps also greatly affects the performance of a model, which means unconstrained attention learning has lots of randomnesses. This uncertainty lets conventional attention learning fall into sub-optimal. In this paper, we propose a compact model to enhance the representational and focusing power of neural attention maps and learn the "inter-attention" correlation for refined attention maps, which we term the "Self-Diversified Multi-Channel Attention Network (SMA-Net)". The proposed method is evaluated on two benchmark databases (BP4D and DISFA) for AU detection and four databases (CK+, MMI, BU-3DFE, and BP4D+) for facial expression recognition. It achieves superior performance compared to the state-of-the-art methods. ',
      categories: ["cs.CV", "cs"],
      title:
        'Your "Attention" Deserves Attention: A Self-Diversified Multi-Channel   Attention for Facial Action Analysis',
      arxiv_id: "2203.12570",
      authors: [
        "Li Zhihua",
        "Yin Lijun",
        "Zhao Geran",
        "Li Xiaotian",
        "Xiaotian Li",
        "Yang Huiyuan",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2022-03-24T00:00:00",
    },
    {
      doi: null,
      submitter: "Jiri Gesi",
      abstract:
        "  Transformer-based models have demonstrated considerable potential for source code modeling tasks in software engineering. However, they are limited by their dependence solely on automatic self-attention weight learning mechanisms. Previous studies have shown that these models overemphasize delimiters added by tokenizers (e.g., [CLS], [SEP]), which may lead to overlooking essential information in the original input source code. To address this challenge, we introduce SyntaGuid, a novel approach that utilizes the observation that attention weights tend to be biased towards specific source code syntax tokens and abstract syntax tree (AST) elements in fine-tuned language models when they make correct predictions. SyntaGuid facilitates the guidance of attention-weight learning, leading to improved model performance on various software engineering tasks. We evaluate the effectiveness of SyntaGuid on multiple tasks and demonstrate that it outperforms existing state-of-the-art models in overall performance without requiring additional data. Experimental result shows that SyntaGuid can improve overall performance up to 3.25 up to 28.3 the attention of Transformer-based models towards critical source code tokens during fine-tuning, highlighting the potential for enhancing Transformer-based models in software engineering. ",
      categories: ["cs.SE", "cs", "cs.AI"],
      title:
        "Beyond Self-learned Attention: Mitigating Attention Bias in   Transformer-based Models Using Attention Guidance",
      arxiv_id: "2402.16790",
      authors: ["Gesi Jiri", "Jiri Gesi", "Ahmed Iftekhar"],
      journal_ref: null,
      comments: "",
      update_date: "2024-02-27T00:00:00",
    },
    {
      doi: null,
      submitter: "Shanshan Wang",
      abstract:
        "  Pre-trained language models (PLM) have demonstrated their effectiveness for a broad range of information retrieval and natural language processing tasks. As the core part of PLM, multi-head self-attention is appealing for its ability to jointly attend to information from different positions. However, researchers have found that PLM always exhibits fixed attention patterns regardless of the input (e.g., excessively paying attention to [CLS] or [SEP]), which we argue might neglect important information in the other positions. In this work, we propose a simple yet effective attention guiding mechanism to improve the performance of PLM by encouraging attention towards the established goals. Specifically, we propose two kinds of attention guiding methods, i.e., map discrimination guiding (MDG) and attention pattern decorrelation guiding (PDG). The former definitely encourages the diversity among multiple self-attention heads to jointly attend to information from different representation subspaces, while the latter encourages self-attention to attend to as many different positions of the input as possible. We conduct experiments with multiple general pre-trained models (i.e., BERT, ALBERT, and Roberta) and domain-specific pre-trained models (i.e., BioBERT, ClinicalBERT, BlueBert, and SciBERT) on three benchmark datasets (i.e., MultiNLI, MedNLI, and Cross-genre-IR). Extensive experimental results demonstrate that our proposed MDG and PDG bring stable performance improvements on all datasets with high efficiency and low cost. ",
      categories: ["cs", "cs.CL"],
      title:
        "Paying More Attention to Self-attention: Improving Pre-trained Language   Models via Attention Guiding",
      arxiv_id: "2204.02922",
      authors: [
        "Yan Qiang",
        "Ren Zhaochun",
        "Shanshan Wang",
        "Chen Zhumin",
        "Wang Shanshan",
        "Ren Pengjie",
        "Liang Huasheng",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2022-04-07T00:00:00",
    },
    {
      doi: null,
      submitter: "Sergey Zagoruyko",
      abstract:
        "  Attention plays a critical role in human visual experience. Furthermore, it has recently been demonstrated that attention can also play an important role in the context of applying artificial neural networks to a variety of tasks from fields such as computer vision and NLP. In this work we show that, by properly defining attention for convolutional neural networks, we can actually use this type of information in order to significantly improve the performance of a student CNN network by forcing it to mimic the attention maps of a powerful teacher network. To that end, we propose several novel methods of transferring attention, showing consistent improvement across a variety of datasets and convolutional neural network architectures. Code and models for our experiments are available at https://github.com/szagoruyko/attention-transfer ",
      categories: ["cs.CV", "cs"],
      title:
        "Paying More Attention to Attention: Improving the Performance of   Convolutional Neural Networks via Attention Transfer",
      arxiv_id: "1612.03928",
      authors: ["Sergey Zagoruyko", "Komodakis Nikos", "Zagoruyko Sergey"],
      journal_ref: null,
      comments: "",
      update_date: "2017-02-14T00:00:00",
    },
    {
      doi: null,
      submitter: "Meiru Zhang",
      abstract:
        "  The context window of large language models has been extended to 128k tokens or more. However, language models still suffer from position bias and have difficulty in accessing and using the middle part of the context due to the lack of attention. We examine the relative position awareness of LLMs and the feasibility of mitigating disproportional attention through prompting. We augment the original task instruction with  that direct language models to allocate more attention towards a selected segment of the context. We conduct a comprehensive investigation on multi-document question answering task with both position-based and index-based instructions. We find that language models do not have relative position awareness of the context. Nevertheless, they demonstrate the capacity to adapt attention to a specific segment using matching indexes. Our analysis contributes to a deeper understanding of position bias in LLMs and provides a pathway to mitigate this bias by instruction, thus benefiting LLMs in locating and utilizing relevant information from retrieved documents in RAG applications. ",
      categories: ["cs", "cs.CL"],
      title:
        "Attention Instruction: Amplifying Attention in the Middle via Prompting",
      arxiv_id: "2406.17095",
      authors: ["Meiru Zhang", "Meng Zaiqiao", "Zhang Meiru", "Collier Nigel"],
      journal_ref: null,
      comments: "",
      update_date: "2024-06-26T00:00:00",
    },
    {
      doi: null,
      submitter: "Yunan Zhang",
      abstract:
        "  Sparse attention, which selectively attends to a subset of tokens in the context was supposed to be efficient. However, its theoretical reduction in FLOPs has rarely translated into wall-clock speed-up over its dense attention counterparts due to the lack of hardware-aware optimizations like FlashAttention. Meanwhile, it remains unclear whether sparse attention can maintain the model's quality at a scale of today's large language models (LLMs) and how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library that provides kernel optimization for sparse attention customizable at both per-head and per-context-range levels. S2-Attention enables the exploration of novel and high-performance sparse attention techniques, which we demonstrate through extensive ablations across a wide range of sparse attention designs at various model scales. From these insights, we present several basic guidelines to design sparse attention that can achieve not only practical efficiency improvements, but also strong downstream performance. To achieve high parallelization and optimized memory IO, sparse attention should shard the context heterogeneously across attention heads, where each head attends to a different subset of tokens while collectively covering the full context. Meanwhile, we find hybrid architectures combining sparse and dense attention particularly beneficial in practice. S2-Attention achieves wall-clock speedup of 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with strong downstream performance on-par with full attention and perfect retrieval performance at a 128k context length. At inference, for 7B models, our model, with the help of our S2-Attention kernel, achieves 4.5x speed-up compared to dense counterparts. S2-Attention is released with easy-to-customize APIs for direct usage in Megatron and vLLM. ",
      categories: ["cs", "cs.CL"],
      title:
        "S2-Attention: Hardware-Aware Context Sharding Among Attention Heads",
      arxiv_id: "2407.17678",
      authors: [
        "Lin Xihui",
        "Patra Barun",
        "Ge Suyu",
        "Yunan Zhang",
        "Peng Hao",
        "Zhang Yunan",
        "Chaudhary Vishrav",
        "Ren Liliang",
        "Song Xia",
      ],
      journal_ref: null,
      comments: "10 pages",
      update_date: "2025-02-06T00:00:00",
    },
    {
      doi: null,
      submitter: "Agniv Sharma",
      abstract:
        "  Transformers are widely used across various applications, many of which yield sparse or partially filled attention matrices. Examples include attention masks designed to reduce the quadratic complexity of attention, sequence packing techniques, and recent innovations like tree masking for fast validation in MEDUSA. Despite the inherent sparsity in these matrices, the state-of-the-art algorithm Flash Attention still processes them with quadratic complexity as though they were dense. In this paper, we introduce Binary Block Masking, a highly efficient modification that enhances Flash Attention by making it mask-aware. We further propose two optimizations: one tailored for masks with contiguous non-zero patterns and another for extremely sparse masks. Our experiments on attention masks derived from real-world scenarios demonstrate up to a 9x runtime improvement. The implementation will be publicly released to foster further research and application. ",
      categories: ["cs.CL", "cs.LG", "cs", "cs.AI"],
      title:
        "Efficiently Dispatching Flash Attention For Partially Filled Attention   Masks",
      arxiv_id: "2409.15097",
      authors: ["Agniv Sharma", "Geiping Jonas", "Sharma Agniv"],
      journal_ref: null,
      comments: "",
      update_date: "2024-09-25T00:00:00",
    },
    {
      doi: null,
      submitter: "Yuanzhi Wang",
      abstract:
        "  Recently, convolutional neural networks (CNNs) have been widely employed to promote the face hallucination due to the ability to predict high-frequency details from a large number of samples. However, most of them fail to take into account the overall facial profile and fine texture details simultaneously, resulting in reduced naturalness and fidelity of the reconstructed face, and further impairing the performance of downstream tasks (e.g., face detection, facial recognition). To tackle this issue, we propose a novel external-internal split attention group (ESAG), which encompasses two paths responsible for facial structure information and facial texture details, respectively. By fusing the features from these two paths, the consistency of facial structure and the fidelity of facial details are strengthened at the same time. Then, we propose a split-attention in split-attention network (SISN) to reconstruct photorealistic high-resolution facial images by cascading several ESAGs. Experimental results on face hallucination and face recognition unveil that the proposed method not only significantly improves the clarity of hallucinated faces, but also encourages the subsequent face recognition performance substantially. Codes have been released at https://github.com/mdswyz/SISN-Face-Hallucination. ",
      categories: ["cs.CV", "cs"],
      title:
        "Face Hallucination via Split-Attention in Split-Attention Network",
      arxiv_id: "2010.11575",
      authors: [
        "Wang Yuanzhi",
        "Lu Tao",
        "Wang Yu",
        "Zhang Yanduo",
        "Wang Zhongyuan",
        "Yuanzhi Wang",
        "Jiang Junjun",
        "Liu Wei",
      ],
      journal_ref: null,
      comments: "Accepted by ACM MM 2021",
      update_date: "2021-07-08T00:00:00",
    },
    {
      doi: null,
      submitter: "Peng Jin",
      abstract:
        "  In this work, we upgrade the multi-head attention mechanism, the core of the Transformer model, to improve efficiency while maintaining or surpassing the previous accuracy level. We show that multi-head attention can be expressed in the summation form. Drawing on the insight that not all attention heads hold equal significance, we propose Mixture-of-Head attention (MoH), a new architecture that treats attention heads as experts in the Mixture-of-Experts (MoE) mechanism. MoH has two significant advantages: First, MoH enables each token to select the appropriate attention heads, enhancing inference efficiency without compromising accuracy or increasing the number of parameters. Second, MoH replaces the standard summation in multi-head attention with a weighted summation, introducing flexibility to the attention mechanism and unlocking extra performance potential. Extensive experiments on ViT, DiT, and LLMs demonstrate that MoH outperforms multi-head attention by using only 50 the attention heads. Moreover, we demonstrate that pre-trained multi-head attention models, such as LLaMA3-8B, can be further continue-tuned into our MoH models. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0 benchmarks, outperforming LLaMA3-8B by 2.4 attention heads. We believe the proposed MoH is a promising alternative to multi-head attention and provides a strong foundation for developing advanced and efficient attention-based models. ",
      categories: ["cs.CV", "cs.LG", "cs", "cs.AI"],
      title: "MoH: Multi-Head Attention as Mixture-of-Head Attention",
      arxiv_id: "2410.11842",
      authors: ["Peng Jin", "Yuan Li", "Zhu Bo", "Yan Shuicheng", "Jin Peng"],
      journal_ref: null,
      comments: "23 pages, code: https://github.com/SkyworkAI/MoH",
      update_date: "2024-10-16T00:00:00",
    },
    {
      doi: null,
      submitter: "Li Rui",
      abstract:
        "  In this paper, to remedy this deficiency, we propose a Linear Attention Mechanism which is approximate to dot-product attention with much less memory and computational costs. The efficient design makes the incorporation between attention mechanisms and neural networks more flexible and versatile. Experiments conducted on semantic segmentation demonstrated the effectiveness of linear attention mechanism. Code is available at https://github.com/lironui/Linear-Attention-Mechanism. ",
      categories: ["cs.CV", "cs"],
      title:
        "Linear Attention Mechanism: An Efficient Attention for Semantic   Segmentation",
      arxiv_id: "2007.14902",
      authors: ["Zheng Shunyi", "Li Rui", "Duan Chenxi", "Su Jianlin"],
      journal_ref: null,
      comments: "",
      update_date: "2020-08-21T00:00:00",
    },
    {
      doi: null,
      submitter: "Bang An",
      abstract:
        "  The neural attention mechanism plays an important role in many natural language processing applications. In particular, the use of multi-head attention extends single-head attention by allowing a model to jointly attend information from different perspectives. Without explicit constraining, however, multi-head attention may suffer from attention collapse, an issue that makes different heads extract similar attentive features, thus limiting the model's representation power. In this paper, for the first time, we provide a novel understanding of multi-head attention from a Bayesian perspective. Based on the recently developed particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves the repulsiveness in multi-head attention and consequently strengthens model's expressiveness. Remarkably, our Bayesian interpretation provides theoretical inspirations on the not-well-understood questions: why and how one uses multi-head attention. Extensive experiments on various attention models and applications demonstrate that the proposed repulsive attention can improve the learned feature diversity, leading to more informative representations with consistent performance improvement on various tasks. ",
      categories: ["stat.ML", "cs.LG", "cs", "stat"],
      title:
        "Repulsive Attention: Rethinking Multi-head Attention as Bayesian   Inference",
      arxiv_id: "2009.09364",
      authors: [
        "Lyu Jie",
        "Tan Fei",
        "Li Chunyuan",
        "Hu Yifan",
        "An Bang",
        "Zhang Ruiyi",
        "Wang Zhenyi",
        "Hu Changwei",
        "Chen Changyou",
        "Bang An",
      ],
      journal_ref: null,
      comments: "accepted by EMNLP2020",
      update_date: "2020-11-03T00:00:00",
    },
    {
      doi: null,
      submitter: "Jiri Hron",
      abstract:
        "  There is a growing amount of literature on the relationship between wide neural networks (NNs) and Gaussian processes (GPs), identifying an equivalence between the two for a variety of NN architectures. This equivalence enables, for instance, accurate approximation of the behaviour of wide Bayesian NNs without MCMC or variational approximations, or characterisation of the distribution of randomly initialised wide NNs optimised by gradient descent without ever running an optimiser. We provide a rigorous extension of these results to NNs involving attention layers, showing that unlike single-head attention, which induces non-Gaussian behaviour, multi-head attention architectures behave as GPs as the number of heads tends to infinity. We further discuss the effects of positional encodings and layer normalisation, and propose modifications of the attention mechanism which lead to improved results for both finite and infinitely wide NNs. We evaluate attention kernels empirically, leading to a moderate improvement upon the previous state-of-the-art on CIFAR-10 for GPs without trainable kernels and advanced data preprocessing. Finally, we introduce new features to the Neural Tangents library (Novak et al., 2020) allowing applications of NNGP/NTK models, with and without attention, to variable-length sequences, with an example on the IMDb reviews dataset. ",
      categories: ["stat.ML", "cs.LG", "cs", "stat"],
      title: "Infinite attention: NNGP and NTK for deep attention networks",
      arxiv_id: "2006.10540",
      authors: [
        "Bahri Yasaman",
        "Hron Jiri",
        "Sohl-Dickstein Jascha",
        "Novak Roman",
        "Jiri Hron",
      ],
      journal_ref: null,
      comments: "ICML 2020",
      update_date: "2020-06-19T00:00:00",
    },
    {
      doi: null,
      submitter: "Jay Heo",
      abstract:
        "  We propose a novel interactive learning framework which we refer to as Interactive Attention Learning (IAL), in which the human supervisors interactively manipulate the allocated attentions, to correct the model's behavior by updating the attention-generating network. However, such a model is prone to overfitting due to scarcity of human annotations, and requires costly retraining. Moreover, it is almost infeasible for the human annotators to examine attentions on tons of instances and features. We tackle these challenges by proposing a sample-efficient attention mechanism and a cost-effective reranking algorithm for instances and features. First, we propose Neural Attention Process (NAP), which is an attention generator that can update its behavior by incorporating new attention-level supervisions without any retraining. Secondly, we propose an algorithm which prioritizes the instances and the features by their negative impacts, such that the model can yield large improvements with minimal human feedback. We validate IAL on various time-series datasets from multiple domains (healthcare, real-estate, and computer vision) on which it significantly outperforms baselines with conventional attention mechanisms, or without cost-effective reranking, with substantially less retraining and human-model interaction cost. ",
      categories: ["stat.ML", "cs.LG", "cs", "stat", "cs.HC"],
      title:
        "Cost-effective Interactive Attention Learning with Neural Attention   Processes",
      arxiv_id: "2006.05419",
      authors: [
        "Kim Kwang Joon",
        "Hwang Sung Ju",
        "Jay Heo",
        "Heo Jay",
        "Yang Eunho",
        "Jeong Hyewon",
        "Park Junhyeon",
        "Lee Juho",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2020-06-11T00:00:00",
    },
    {
      doi: null,
      submitter: "Michael Yeung",
      abstract:
        "  In recent years, there has been increasing interest to incorporate attention into deep learning architectures for biomedical image segmentation. The modular design of attention mechanisms enables flexible integration into convolutional neural network architectures, such as the U-Net. Whether attention is appropriate to use, what type of attention to use, and where in the network to incorporate attention modules, are all important considerations that are currently overlooked. In this paper, we investigate the role of the Focal parameter in modulating attention, revealing a link between attention in loss functions and networks. By incorporating a Focal distance penalty term, we extend the Unified Focal loss framework to include boundary-based losses. Furthermore, we develop a simple and interpretable, dataset and model-specific heuristic to integrate the Focal parameter into the Squeeze-and-Excitation block and Attention Gate, achieving optimal performance with fewer number of attention modules on three well-validated biomedical imaging datasets, suggesting judicious use of attention modules results in better performance and efficiency. ",
      categories: [
        "cs.CV",
        "eess.IV",
        "cs",
        "eess",
        "cs.AI",
        "math",
        "math.OC",
      ],
      title:
        "Focal Attention Networks: optimising attention for biomedical image   segmentation",
      arxiv_id: "2111.00534",
      authors: [
        "Michael Yeung",
        "Sch√∂nlieb Carola-Bibiane",
        "Rundo Leonardo",
        "Sala Evis",
        "Yeung Michael",
        "Yang Guang",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2021-11-02T00:00:00",
    },
    {
      doi: null,
      submitter: "Jasdeep Singh",
      abstract:
        "  Visual Question Answering (VQA) is an increasingly popular topic in deep learning research, requiring coordination of natural language processing and computer vision modules into a single architecture. We build upon the model which placed first in the VQA Challenge by developing thirteen new attention mechanisms and introducing a simplified classifier. We performed 300 GPU hours of extensive hyperparameter and architecture searches and were able to achieve an evaluation score of 64.78 single model's validation score of 63.15 ",
      categories: ["cs.CV", "cs.AI", "cs", "cs.CL"],
      title:
        "Attention on Attention: Architectures for Visual Question Answering   (VQA)",
      arxiv_id: "1803.07724",
      authors: [
        "Jasdeep Singh",
        "Nutkiewicz Alex",
        "Singh Jasdeep",
        "Ying Vincent",
      ],
      journal_ref: null,
      comments: "Visual Question Answering Project",
      update_date: "2018-03-22T00:00:00",
    },
    {
      doi: null,
      submitter: "Robert Huben",
      abstract:
        "  The transformer architecture is widely used in machine learning models and consists of two alternating sublayers: attention heads and MLPs. We prove that an MLP neuron can be implemented by a masked attention head with internal dimension 1 so long as the MLP's activation function comes from a restricted class including SiLU and close approximations of ReLU and GeLU. This allows one to convert an MLP-and-attention transformer into an attention-only transformer at the cost of greatly increasing the number of attention heads. We also prove that attention heads can perform the components of an MLP (linear transformations and activation functions) separately. Finally, we prove that attention heads can encode arbitrary masking patterns in their weight matrices to within arbitrarily small error. ",
      categories: ["cs.LG", "cs"],
      title:
        "Attention-Only Transformers and Implementing MLPs with Attention Heads",
      arxiv_id: "2309.08593",
      authors: ["Robert Huben", "Morris Valerie", "Huben Robert"],
      journal_ref: null,
      comments: "11 pages",
      update_date: "2023-09-18T00:00:00",
    },
    {
      doi: null,
      submitter: "Xiaofeng Zhang",
      abstract:
        "  Mixture-of-Experts (MoE) networks have been proposed as an efficient way to scale up model capacity and implement conditional computing. However, the study of MoE components mostly focused on the feedforward layer in Transformer architecture. This paper proposes the Mixture of Attention Heads (MoA), a new architecture that combines multi-head attention with the MoE mechanism. MoA includes a set of attention heads that each has its own set of parameters. Given an input, a router dynamically selects a subset of k attention heads per token. This conditional computation schema allows MoA to achieve stronger performance than the standard multi-head attention layer. Furthermore, the sparsely gated MoA can easily scale up the number of attention heads and the number of parameters while preserving computational efficiency. In addition to the performance improvements, MoA also automatically differentiates heads' utilities, providing a new perspective to discuss the model's interpretability. We conducted experiments on several important tasks, including Machine Translation and Masked Language Modeling. Experiments have shown promising results on several tasks against strong baselines that involve large and very deep models. ",
      categories: ["cs", "cs.CL"],
      title: "Mixture of Attention Heads: Selecting Attention Heads Per Token",
      arxiv_id: "2210.05144",
      authors: [
        "Zhang Xiaofeng",
        "Huang Zeyu",
        "Xiong Zhang",
        "Shen Yikang",
        "Xiaofeng Zhang",
        "Zhou Jie",
        "Rong Wenge",
      ],
      journal_ref: null,
      comments: "accepted in EMNLP 2022",
      update_date: "2022-10-12T00:00:00",
    },
    {
      doi: null,
      submitter: "Shashanka Venkataramanan",
      abstract:
        "  This work aims to improve the efficiency of vision transformers (ViT). While ViTs use computationally expensive self-attention operations in every layer, we identify that these operations are highly correlated across layers ‚Äì a key redundancy that causes unnecessary computations. Based on this observation, we propose SkipAt, a method to reuse self-attention computation from preceding layers to approximate attention at one or more subsequent layers. To ensure that reusing self-attention blocks across layers does not degrade the performance, we introduce a simple parametric function, which outperforms the baseline transformer's performance while running computationally faster. We show the effectiveness of our method in image classification and self-supervised learning on ImageNet-1K, semantic segmentation on ADE20K, image denoising on SIDD, and video denoising on DAVIS. We achieve improved throughput at the same-or-higher accuracy levels in all these tasks. ",
      categories: ["cs.CV", "cs"],
      title:
        "Skip-Attention: Improving Vision Transformers by Paying Less Attention",
      arxiv_id: "2301.02240",
      authors: [
        "Venkataramanan Shashanka",
        "Ghodrati Amir",
        "Shashanka Venkataramanan",
        "Asano Yuki M.",
        "Habibian Amirhossein",
        "Porikli Fatih",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2023-01-18T00:00:00",
    },
    {
      doi: null,
      submitter: "Mootez Saad",
      abstract:
        "  Language models for code such as CodeBERT offer the capability to learn advanced source code representation, but their opacity poses barriers to understanding of captured properties. Recent attention analysis studies provide initial interpretability insights by focusing solely on attention weights rather than considering the wider context modeling of Transformers. This study aims to shed some light on the previously ignored factors of the attention mechanism beyond the attention weights. We conduct an initial empirical study analyzing both attention distributions and transformed representations in CodeBERT. Across two programming languages, Java and Python, we find that the scaled transformation norms of the input better capture syntactic structure compared to attention weights alone. Our analysis reveals characterization of how CodeBERT embeds syntactic code properties. The findings demonstrate the importance of incorporating factors beyond just attention weights for rigorously understanding neural code models. This lays the groundwork for developing more interpretable models and effective uses of attention mechanisms in program analysis. ",
      categories: ["cs.LG", "cs.SE", "cs"],
      title:
        "Naturalness of Attention: Revisiting Attention in Code Language Models",
      arxiv_id: "2311.13508",
      authors: ["Saad Mootez", "Mootez Saad", "Sharma Tushar"],
      journal_ref: null,
      comments: "Accepted at ICSE-NIER (2024) track",
      update_date: "2023-11-23T00:00:00",
    },
    {
      doi: null,
      submitter: "Or Patashnik",
      abstract:
        "  Personalizing text-to-image models to generate images of specific subjects across diverse scenes and styles is a rapidly advancing field. Current approaches often face challenges in maintaining a balance between identity preservation and alignment with the input text prompt. Some methods rely on a single textual token to represent a subject, which limits expressiveness, while others employ richer representations but disrupt the model's prior, diminishing prompt alignment. In this work, we introduce Nested Attention, a novel mechanism that injects a rich and expressive image representation into the model's existing cross-attention layers. Our key idea is to generate query-dependent subject values, derived from nested attention layers that learn to select relevant subject features for each region in the generated image. We integrate these nested layers into an encoder-based personalization method, and show that they enable high identity preservation while adhering to input text prompts. Our approach is general and can be trained on various domains. Additionally, its prior preservation allows us to combine multiple personalized subjects from different domains in a single image. ",
      categories: ["cs.CV", "cs.GR", "cs.LG", "cs"],
      title:
        "Nested Attention: Semantic-aware Attention Values for Concept   Personalization",
      arxiv_id: "2501.01407",
      authors: [
        "Ostashev Daniil",
        "Patashnik Or",
        "Or Patashnik",
        "Aberman Kfir",
        "Gal Rinon",
        "Tulyakov Sergey",
        "Cohen-Or Daniel",
      ],
      journal_ref: null,
      comments:
        "Project page at https://snap-research.github.io/NestedAttention/",
      update_date: "2025-01-03T00:00:00",
    },
    {
      doi: null,
      submitter: "Aaron Serianni",
      abstract:
        "  Computer vision models have been shown to exhibit and amplify biases across a wide array of datasets and tasks. Existing methods for quantifying bias in classification models primarily focus on dataset distribution and model performance on subgroups, overlooking the internal workings of a model. We introduce the Attention-IoU (Attention Intersection over Union) metric and related scores, which use attention maps to reveal biases within a model's internal representations and identify image features potentially causing the biases. First, we validate Attention-IoU on the synthetic Waterbirds dataset, showing that the metric accurately measures model bias. We then analyze the CelebA dataset, finding that Attention-IoU uncovers correlations beyond accuracy disparities. Through an investigation of individual attributes through the protected attribute of Male, we examine the distinct ways biases are represented in CelebA. Lastly, by subsampling the training set to change attribute correlations, we demonstrate that Attention-IoU reveals potential confounding variables not present in dataset labels. ",
      categories: ["cs.CV", "cs.LG", "cs"],
      title: "Attention IoU: Examining Biases in CelebA using Attention Maps",
      arxiv_id: "2503.19846",
      authors: [
        "Serianni Aaron",
        "Russakovsky Olga",
        "Zhu Tyler",
        "Aaron Serianni",
        "Ramaswamy Vikram V.",
      ],
      journal_ref: null,
      comments:
        "To appear in CVPR 2025. Code and data is available at   https://github.com/aaronserianni/attention-iou . 15 pages, 14 figures,   including appendix",
      update_date: "2025-03-27T00:00:00",
    },
    {
      doi: null,
      submitter: "Pedram Zaree",
      abstract:
        "  Recent research has shown that carefully crafted jailbreak inputs can induce large language models to produce harmful outputs, despite safety measures such as alignment. It is important to anticipate the range of potential Jailbreak attacks to guide effective defenses and accurate assessment of model safety. In this paper, we present a new approach for generating highly effective Jailbreak attacks that manipulate the attention of the model to selectively strengthen or weaken attention among different parts of the prompt. By harnessing attention loss, we develop more effective jailbreak attacks, that are also transferrable. The attacks amplify the success rate of existing Jailbreak algorithms including GCG, AutoDAN, and ReNeLLM, while lowering their generation cost (for example, the amplified GCG attack achieves 91.2 on Llama2-7B/AdvBench, using less than a third of the generation time). ",
      categories: ["cs.AI", "cs.LG", "cs", "cs.CR"],
      title:
        "Attention Eclipse: Manipulating Attention to Bypass LLM Safety-Alignment",
      arxiv_id: "2502.15334",
      authors: [
        "Dong Yue",
        "Abu-Ghazaleh Nael",
        "Zaree Pedram",
        "Pedram Zaree",
        "Alouani Ihsen",
        "Mamun Md Abdullah Al",
        "Alam Quazi Mishkatul",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2025-02-24T00:00:00",
    },
    {
      doi: null,
      submitter: "Ali Hassani",
      abstract:
        "  We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision. NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike Swin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package with efficient C++ and CUDA kernels, which allows NA to run up to 40 less memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA that boosts image classification and downstream vision performance. Experimental results on NAT are competitive; NAT-Tiny reaches 83.2 48.4 ADE20K mIoU improvement over a Swin model with similar size. To support more research based on sliding-window attention, we open source our project and release our checkpoints at: https://github.com/SHI-Labs/Neighborhood-Attention-Transformer . ",
      categories: ["cs.CV", "cs.LG", "cs", "cs.AI"],
      title: "Neighborhood Attention Transformer",
      arxiv_id: "2204.07143",
      authors: [
        "Li Shen",
        "Ali Hassani",
        "Hassani Ali",
        "Shi Humphrey",
        "Li Jiachen",
        "Walton Steven",
      ],
      journal_ref: null,
      comments:
        "To appear in CVPR 2023. NATTEN is open-sourced at:   https://github.com/SHI-Labs/NATTEN/",
      update_date: "2023-05-18T00:00:00",
    },
    {
      doi: null,
      submitter: "Daniel Glickman",
      abstract:
        "  The dominant paradigm for machine learning on graphs uses Message Passing Graph Neural Networks (MP-GNNs), in which node representations are updated by aggregating information in their local neighborhood. Recently, there have been increasingly more attempts to adapt the Transformer architecture to graphs in an effort to solve some known limitations of MP-GNN. A challenging aspect of designing Graph Transformers is integrating the arbitrary graph structure into the architecture. We propose Graph Diffuser (GD) to address this challenge. GD learns to extract structural and positional relationships between distant nodes in the graph, which it then uses to direct the Transformer's attention and node representation. We demonstrate that existing GNNs and Graph Transformers struggle to capture long-range interactions and how Graph Diffuser does so while admitting intuitive visualizations. Experiments on eight benchmarks show Graph Diffuser to be a highly competitive model, outperforming the state-of-the-art in a diverse set of domains. ",
      categories: ["cs.LG", "cs"],
      title: "Diffusing Graph Attention",
      arxiv_id: "2303.00613",
      authors: ["Glickman Daniel", "Yahav Eran", "Daniel Glickman"],
      journal_ref: null,
      comments: "",
      update_date: "2023-03-02T00:00:00",
    },
    {
      doi: null,
      submitter: "Cristian Bodnar",
      abstract:
        "  Graph representation learning methods have mostly been limited to the modelling of node-wise interactions. Recently, there has been an increased interest in understanding how higher-order structures can be utilised to further enhance the learning abilities of graph neural networks (GNNs) in combinatorial spaces. Simplicial Neural Networks (SNNs) naturally model these interactions by performing message passing on simplicial complexes, higher-dimensional generalisations of graphs. Nonetheless, the computations performed by most existent SNNs are strictly tied to the combinatorial structure of the complex. Leveraging the success of attention mechanisms in structured domains, we propose Simplicial Attention Networks (SAT), a new type of simplicial network that dynamically weighs the interactions between neighbouring simplicies and can readily adapt to novel structures. Additionally, we propose a signed attention mechanism that makes SAT orientation equivariant, a desirable property for models operating on (co)chain complexes. We demonstrate that SAT outperforms existent convolutional SNNs and GNNs in two image and trajectory classification tasks. ",
      categories: ["math.AT", "cs.LG", "cs", "math"],
      title: "Simplicial Attention Networks",
      arxiv_id: "2204.09455",
      authors: [
        "Cristian Bodnar",
        "Bodnar Cristian",
        "Li√≤ Pietro",
        "Goh Christopher Wei Jin",
      ],
      journal_ref: null,
      comments:
        "Accepted to the ICLR 2022 Workshop on Geometrical and Topological   Representation Learning",
      update_date: "2022-04-21T00:00:00",
    },
    {
      doi: null,
      submitter: "Meng Zheng",
      abstract:
        "  While there has been substantial progress in learning suitable distance metrics, these techniques in general lack transparency and decision reasoning, i.e., explaining why the input set of images is similar or dissimilar. In this work, we solve this key problem by proposing the first method to generate generic visual similarity explanations with gradient-based attention. We demonstrate that our technique is agnostic to the specific similarity model type, e.g., we show applicability to Siamese, triplet, and quadruplet models. Furthermore, we make our proposed similarity attention a principled part of the learning process, resulting in a new paradigm for learning similarity functions. We demonstrate that our learning mechanism results in more generalizable, as well as explainable, similarity models. Finally, we demonstrate the generality of our framework by means of experiments on a variety of tasks, including image retrieval, person re-identification, and low-shot semantic segmentation. ",
      categories: ["cs.CV", "cs.LG", "cs"],
      title: "Visual Similarity Attention",
      arxiv_id: "1911.07381",
      authors: [
        "Radke Richard J.",
        "Meng Zheng",
        "Zheng Meng",
        "Wu Ziyan",
        "Karanam Srikrishna",
        "Chen Terrence",
      ],
      journal_ref: null,
      comments: "10 pages, 7 figures, 4 tables",
      update_date: "2022-05-05T00:00:00",
    },
    {
      doi: null,
      submitter: "Shenghao Yang",
      abstract:
        '  Graph-based learning is a rapidly growing sub-field of machine learning with applications in social networks, citation networks, and bioinformatics. One of the most popular models is graph attention networks. They were introduced to allow a node to aggregate information from features of neighbor nodes in a non-uniform way, in contrast to simple graph convolution which does not distinguish the neighbors of a node. In this paper, we theoretically study the behaviour of graph attention networks. We prove multiple results on the performance of the graph attention mechanism for the problem of node classification for a contextual stochastic block model. Here, the node features are obtained from a mixture of Gaussians and the edges from a stochastic block model. We show that in an "easy" regime, where the distance between the means of the Gaussians is large enough, graph attention is able to distinguish inter-class from intra-class edges. Thus it maintains the weights of important edges and significantly reduces the weights of unimportant edges. Consequently, we show that this implies perfect node classification. In the "hard" regime, we show that every attention mechanism fails to distinguish intra-class from inter-class edges. In addition, we show that graph attention convolution cannot (almost) perfectly classify the nodes even if intra-class edges could be separated from inter-class edges. Beyond perfect node classification, we provide a positive result on graph attention\'s robustness against structural noise in the graph. In particular, our robustness result implies that graph attention can be strictly better than both the simple graph convolution and the best linear classifier of node features. We evaluate our theoretical results on synthetic and real-world data. ',
      categories: ["stat.ML", "cs.LG", "cs", "stat"],
      title: "Graph Attention Retrospective",
      arxiv_id: "2202.13060",
      authors: [
        "Jagannath Aukosh",
        "Fountoulakis Kimon",
        "Shenghao Yang",
        "Yang Shenghao",
        "Levi Amit",
        "Baranwal Aseem",
      ],
      journal_ref: null,
      comments: "45 pages, 5 figures",
      update_date: "2023-05-23T00:00:00",
    },
    {
      doi: null,
      submitter: "Kyungwoo Song",
      abstract:
        "  Attention computes the dependency between representations, and it encourages the model to focus on the important selective features. Attention-based models, such as Transformer and graph attention network (GAT), are widely utilized for sequential data and graph-structured data. This paper suggests a new interpretation and generalized structure of the attention in Transformer and GAT. For the attention in Transformer and GAT, we derive that the attention is a product of two parts: 1) the RBF kernel to measure the similarity of two instances and 2) the exponential of L^2 norm to compute the importance of individual instances. From this decomposition, we generalize the attention in three ways. First, we propose implicit kernel attention with an implicit kernel function instead of manual kernel selection. Second, we generalize L^2 norm as the L^p norm. Third, we extend our attention to structured multi-head attention. Our generalized attention shows better performance on classification, translation, and regression tasks. ",
      categories: ["stat.ML", "cs.LG", "cs", "stat"],
      title: "Implicit Kernel Attention",
      arxiv_id: "2006.06147",
      authors: [
        "Song Kyungwoo",
        "Moon Il-Chul",
        "Kim Dongjun",
        "Kyungwoo Song",
        "Jung Yohan",
      ],
      journal_ref: null,
      comments: "AAAI-21",
      update_date: "2021-03-02T00:00:00",
    },
    {
      doi: null,
      submitter: "Xinjie Fan",
      abstract:
        "  Attention modules, as simple and effective tools, have not only enabled deep neural networks to achieve state-of-the-art results in many domains, but also enhanced their interpretability. Most current models use deterministic attention modules due to their simplicity and ease of optimization. Stochastic counterparts, on the other hand, are less popular despite their potential benefits. The main reason is that stochastic attention often introduces optimization issues or requires significant model changes. In this paper, we propose a scalable stochastic version of attention that is easy to implement and optimize. We construct simplex-constrained attention distributions by normalizing reparameterizable distributions, making the training process differentiable. We learn their parameters in a Bayesian framework where a data-dependent prior is introduced for regularization. We apply the proposed stochastic attention modules to various attention-based models, with applications to graph node classification, visual question answering, image captioning, machine translation, and language understanding. Our experiments show the proposed method brings consistent improvements over the corresponding baselines. ",
      categories: ["stat.ML", "cs.LG", "cs", "stat", "cs.NE"],
      title: "Bayesian Attention Modules",
      arxiv_id: "2010.10604",
      authors: [
        "Chen Bo",
        "Zhou Mingyuan",
        "Xinjie Fan",
        "Zhang Shujian",
        "Fan Xinjie",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2020-10-22T00:00:00",
    },
    {
      doi: null,
      submitter: "Tu Vo",
      abstract:
        "  We develop a deep convolutional neural networks(CNNs) to deal with the blurry artifacts caused by the defocus of the camera using dual-pixel images. Specifically, we develop a double attention network which consists of attentional encoders, triple locals and global local modules to effectively extract useful information from each image in the dual-pixels and select the useful information from each image and synthesize the final output image. We demonstrate the effectiveness of the proposed deblurring algorithm in terms of both qualitative and quantitative aspects by evaluating on the test set in the NTIRE 2021 Defocus Deblurring using Dual-pixel Images Challenge. The code, and trained models are available at https://github.com/tuvovan/ATTSF. ",
      categories: ["cs.CV", "eess", "eess.IV", "cs"],
      title: "Attention! Stay Focus!",
      arxiv_id: "2104.07925",
      authors: ["Vo Tu", "Tu Vo"],
      journal_ref: null,
      comments: "",
      update_date: "2021-04-19T00:00:00",
    },
    {
      doi: null,
      submitter: "Dazhuo Wei",
      abstract:
        "  In this paper, I introduce a random attention span model (RAS) which uses stopping time to identify decision-makers' behavior under limited attention. Unlike many limited attention models, the RAS identifies preferences using time variation without any need for menu variation. In addition, the RAS allows the consideration set to be correlated with the preference. I also use the revealed preference theory that provides testable implications for observable choice probabilities. Then, I test the model and estimate the preference distribution using data from M-Turk experiments on choice behaviors that involve lotteries; there is general alignment with the distribution results from logit attention model. ",
      categories: ["econ.TH", "econ"],
      title: "Random Attention Span",
      arxiv_id: "2405.11578",
      authors: ["Wei Dazhuo", "Dazhuo Wei"],
      journal_ref: null,
      comments: "",
      update_date: "2024-05-21T00:00:00",
    },
    {
      doi: "10.1145/3394486.3403065",
      submitter: "Hongyang Gao",
      abstract:
        "  Attention operators have been applied on both 1-D data like texts and higher-order data such as images and videos. Use of attention operators on high-order data requires flattening of the spatial or spatial-temporal dimensions into a vector, which is assumed to follow a multivariate normal distribution. This not only incurs excessive requirements on computational resources, but also fails to preserve structures in data. In this work, we propose to avoid flattening by assuming the data follow matrix-variate normal distributions. Based on this new view, we develop Kronecker attention operators (KAOs) that operate on high-order tensor data directly. More importantly, the proposed KAOs lead to dramatic reductions in computational resources. Experimental results show that our methods reduce the amount of required computational resources by a factor of hundreds, with larger factors for higher-dimensional and higher-order data. Results also show that networks with KAOs outperform models without attention, while achieving competitive performance as those with original attention operators. ",
      categories: ["cs.CV", "cs.LG", "cs"],
      title: "Kronecker Attention Networks",
      arxiv_id: "2007.08442",
      authors: [
        "Ji Shuiwang",
        "Wang Zhengyang",
        "Hongyang Gao",
        "Gao Hongyang",
      ],
      journal_ref: null,
      comments: "9 pages, KDD2020",
      update_date: "2020-07-17T00:00:00",
    },
    {
      doi: null,
      submitter: "Noam Shazeer",
      abstract:
        '  We introduce "talking-heads attention" - a variation on multi-head attention which includes linearprojections across the attention-heads dimension, immediately before and after the softmax operation.While inserting only a small number of additional parameters and a moderate amount of additionalcomputation, talking-heads attention leads to better perplexities on masked language modeling tasks, aswell as better quality when transfer-learning to language comprehension and question answering tasks. ',
      categories: [
        "stat.ML",
        "cs.LG",
        "cs",
        "eess.AS",
        "eess",
        "cs.SD",
        "stat",
        "cs.NE",
      ],
      title: "Talking-Heads Attention",
      arxiv_id: "2003.02436",
      authors: [
        "Cheng Youlong",
        "Hou Le",
        "Ding Nan",
        "Lan Zhenzhong",
        "Shazeer Noam",
        "Noam Shazeer",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2020-03-06T00:00:00",
    },
    {
      doi: null,
      submitter: "Dongyan Guo",
      abstract:
        "  Siamese network based trackers formulate the visual tracking task as a similarity matching problem. Almost all popular Siamese trackers realize the similarity learning via convolutional feature cross-correlation between a target branch and a search branch. However, since the size of target feature region needs to be pre-fixed, these cross-correlation base methods suffer from either reserving much adverse background information or missing a great deal of foreground information. Moreover, the global matching between the target and search region also largely neglects the target structure and part-level information.   In this paper, to solve the above issues, we propose a simple target-aware Siamese graph attention network for general object tracking. We propose to establish part-to-part correspondence between the target and the search region with a complete bipartite graph, and apply the graph attention mechanism to propagate target information from the template feature to the search feature. Further, instead of using the pre-fixed region cropping for template-feature-area selection, we investigate a target-aware area selection mechanism to fit the size and aspect ratio variations of different objects. Experiments on challenging benchmarks including GOT-10k, UAV123, OTB-100 and LaSOT demonstrate that the proposed SiamGAT outperforms many state-of-the-art trackers and achieves leading performance. Code is available at: https://git.io/SiamGAT ",
      categories: ["cs.CV", "cs"],
      title: "Graph Attention Tracking",
      arxiv_id: "2011.11204",
      authors: [
        "Cui Ying",
        "Shen Chunhua",
        "Dongyan Guo",
        "Zhang Liyan",
        "Wang Zhenhua",
        "Shao Yanyan",
        "Guo Dongyan",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2020-11-24T00:00:00",
    },
    {
      doi: null,
      submitter: "Alexander M. Rush",
      abstract:
        "  Transformers have been essential to pretraining success in NLP. While other architectures have been used, downstream accuracy is either significantly worse, or requires attention layers to match standard benchmarks such as GLUE. This work explores pretraining without attention by using recent advances in sequence routing based on state-space models (SSMs). Our proposed model, Bidirectional Gated SSM (BiGS), combines SSM layers with a multiplicative gating architecture that has been effective in simplified sequence modeling architectures. The model learns static layers that do not consider pair-wise interactions. Even so, BiGS is able to match BERT pretraining accuracy on GLUE and can be extended to long-form pretraining of 4096 tokens without approximation. Analysis shows that while the models have similar average accuracy, the approach has different inductive biases than BERT in terms of interactions and syntactic representations. All models from this work are available at https://github.com/jxiw/BiGS. ",
      categories: ["cs.LG", "cs", "cs.CL"],
      title: "Pretraining Without Attention",
      arxiv_id: "2212.10544",
      authors: [
        "Wang Junxiong",
        "Alexander M. Rush",
        "Yan Jing Nathan",
        "Gu Albert",
        "Rush Alexander M.",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2023-05-10T00:00:00",
    },
    {
      doi: null,
      submitter: "Hyoungwook Nam",
      abstract:
        "  We propose a novel perspective of the attention mechanism by reinventing it as a memory architecture for neural networks, namely Neural Attention Memory (NAM). NAM is a memory structure that is both readable and writable via differentiable linear algebra operations. We explore three use cases of NAM: memory-augmented neural network (MANN), few-shot learning, and efficient long-range attention. First, we design two NAM-based MANNs of Long Short-term Memory (LSAM) and NAM Turing Machine (NAM-TM) that show better computational powers in algorithmic zero-shot generalization tasks compared to other baselines such as differentiable neural computer (DNC). Next, we apply NAM to the N-way K-shot learning task and show that it is more effective at reducing false positives compared to the baseline cosine classifier. Finally, we implement an efficient Transformer with NAM and evaluate it with long-range arena tasks to show that NAM can be an efficient and effective alternative for scaled dot-product attention. ",
      categories: ["cs.LG", "cs"],
      title: "Neural Attention Memory",
      arxiv_id: "2302.09422",
      authors: ["Hyoungwook Nam", "Seo Seung Byum", "Nam Hyoungwook"],
      journal_ref: null,
      comments: "Preprint. Under review",
      update_date: "2023-10-17T00:00:00",
    },
    {
      doi: null,
      submitter: "{\\L}ukasz Maziarka",
      abstract:
        "  Designing a single neural network architecture that performs competitively across a range of molecule property prediction tasks remains largely an open challenge, and its solution may unlock a widespread use of deep learning in the drug discovery industry. To move towards this goal, we propose Molecule Attention Transformer (MAT). Our key innovation is to augment the attention mechanism in Transformer using inter-atomic distances and the molecular graph structure. Experiments show that MAT performs competitively on a diverse set of molecular prediction tasks. Most importantly, with a simple self-supervised pretraining, MAT requires tuning of only a few hyperparameter values to achieve state-of-the-art performance on downstream tasks. Finally, we show that attention weights learned by MAT are interpretable from the chemical point of view. ",
      categories: [
        "stat.ML",
        "cs.LG",
        "cs",
        "stat",
        "physics.comp-ph",
        "physics",
      ],
      title: "Molecule Attention Transformer",
      arxiv_id: "2002.08264",
      authors: [
        "Mucha S≈Çawomir",
        "Jastrzƒôbski Stanis≈Çaw",
        "Danel Tomasz",
        "Maziarka ≈Åukasz",
        "Tabor Jacek",
        "Rataj Krzysztof",
        "{\\L}ukasz Maziarka",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2021-02-10T00:00:00",
    },
    {
      doi: null,
      submitter: "Yi Tay",
      abstract:
        "  We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers. ",
      categories: ["cs.LG", "cs", "cs.CL"],
      title: "Sparse Sinkhorn Attention",
      arxiv_id: "2002.11296",
      authors: [
        "Juan Da-Cheng",
        "Metzler Donald",
        "Yi Tay",
        "Yang Liu",
        "Bahri Dara",
        "Tay Yi",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2020-02-27T00:00:00",
    },
    {
      doi: null,
      submitter: "Shashank Sonkar",
      abstract:
        "  Word embedding models learn semantically rich vector representations of words and are widely used to initialize natural processing language (NLP) models. The popular continuous bag-of-words (CBOW) model of word2vec learns a vector embedding by masking a given word in a sentence and then using the other words as a context to predict it. A limitation of CBOW is that it equally weights the context words when making a prediction, which is inefficient, since some words have higher predictive value than others. We tackle this inefficiency by introducing the Attention Word Embedding (AWE) model, which integrates the attention mechanism into the CBOW model. We also propose AWE-S, which incorporates subword information. We demonstrate that AWE and AWE-S outperform the state-of-the-art word embedding models both on a variety of word similarity datasets and when used for initialization of NLP models. ",
      categories: ["cs.LG", "cs", "cs.CL"],
      title: "Attention Word Embedding",
      arxiv_id: "2006.00988",
      authors: [
        "Waters Andrew E.",
        "Baraniuk Richard G.",
        "Sonkar Shashank",
        "Shashank Sonkar",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2020-06-02T00:00:00",
    },
    {
      doi: null,
      submitter: "Yimian Dai",
      abstract:
        "  Activation functions and attention mechanisms are typically treated as having different purposes and have evolved differently. However, both concepts can be formulated as a non-linear gating function. Inspired by their similarity, we propose a novel type of activation units called attentional activation (ATAC) units as a unification of activation functions and attention mechanisms. In particular, we propose a local channel attention module for the simultaneous non-linear activation and element-wise feature refinement, which locally aggregates point-wise cross-channel feature contexts. By replacing the well-known rectified linear units by such ATAC units in convolutional networks, we can construct fully attentional networks that perform significantly better with a modest number of additional parameters. We conducted detailed ablation studies on the ATAC units using several host networks with varying network depths to empirically verify the effectiveness and efficiency of the units. Furthermore, we compared the performance of the ATAC units against existing activation functions as well as other attention mechanisms on the CIFAR-10, CIFAR-100, and ImageNet datasets. Our experimental results show that networks constructed with the proposed ATAC units generally yield performance gains over their competitors given a comparable number of parameters. ",
      categories: ["cs.CV", "cs"],
      title: "Attention as Activation",
      arxiv_id: "2007.07729",
      authors: [
        "Wu Yiquan",
        "Dai Yimian",
        "Barnard Kobus",
        "Yimian Dai",
        "Gieseke Fabian",
        "Oehmcke Stefan",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2020-08-04T00:00:00",
    },
    {
      doi: null,
      submitter: "Raviteja Chunduru",
      abstract:
        "  Temporal abstraction in reinforcement learning is the ability of an agent to learn and use high-level behaviors, called options. The option-critic architecture provides a gradient-based end-to-end learning method to construct options. We propose an attention-based extension to this framework, which enables the agent to learn to focus different options on different aspects of the observation space. We show that this leads to behaviorally diverse options which are also capable of state abstraction, and prevents the degeneracy problems of option domination and frequent option switching that occur in option-critic, while achieving a similar sample complexity. We also demonstrate the more efficient, interpretable, and reusable nature of the learned options in comparison with option-critic, through different transfer learning tasks. Experimental results in a relatively simple four-rooms environment and the more complex ALE (Arcade Learning Environment) showcase the efficacy of our approach. ",
      categories: ["cs.LG", "cs", "cs.AI"],
      title: "Attention Option-Critic",
      arxiv_id: "2201.02628",
      authors: ["Precup Doina", "Raviteja Chunduru", "Chunduru Raviteja"],
      journal_ref: null,
      comments: "",
      update_date: "2022-01-11T00:00:00",
    },
    {
      doi: null,
      submitter: "Meng-Hao Guo",
      abstract:
        "  While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel linear attention named large kernel attention (LKA) to enable self-adaptive and long-range correlations in self-attention while avoiding its shortcomings. Furthermore, we present a neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN surpasses similar size vision transformers(ViTs) and convolutional neural networks(CNNs) in various tasks, including image classification, object detection, semantic segmentation, panoptic segmentation, pose estimation, etc. For example, VAN-B6 achieves 87.8 benchmark and set new state-of-the-art performance (58.2 PQ) for panoptic segmentation. Besides, VAN-B2 surpasses Swin-T 4 semantic segmentation on ADE20K benchmark, 2.6 detection on COCO dataset. It provides a novel method and a simple yet strong baseline for the community. Code is available at https://github.com/Visual-Attention-Network. ",
      categories: ["cs.CV", "cs"],
      title: "Visual Attention Network",
      arxiv_id: "2202.09741",
      authors: [
        "Meng-Hao Guo",
        "Lu Cheng-Ze",
        "Hu Shi-Min",
        "Guo Meng-Hao",
        "Cheng Ming-Ming",
        "Liu Zheng-Ning",
      ],
      journal_ref: null,
      comments:
        "Code is available at https://github.com/Visual-Attention-Network",
      update_date: "2022-07-12T00:00:00",
    },
    {
      doi: null,
      submitter: "Petar Veli\\v{c}kovi\\'c",
      abstract:
        "  We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training). ",
      categories: ["stat.ML", "cs.LG", "cs", "cs.SI", "cs.AI", "stat"],
      title: "Graph Attention Networks",
      arxiv_id: "1710.10903",
      authors: [
        "Veliƒçkoviƒá Petar",
        "Cucurull Guillem",
        "Casanova Arantxa",
        "Petar Veli\\v{c}kovi\\'c",
        "Romero Adriana",
        "Bengio Yoshua",
        "Li√≤ Pietro",
      ],
      journal_ref: null,
      comments: "To appear at ICLR 2018. 12 pages, 2 figures",
      update_date: "2018-02-06T00:00:00",
    },
    {
      doi: null,
      submitter: "Chung-Cheng Chiu",
      abstract:
        "  Sequence-to-sequence models with soft attention have been successfully applied to a wide variety of problems, but their decoding process incurs a quadratic time and space cost and is inapplicable to real-time sequence transduction. To address these issues, we propose Monotonic Chunkwise Attention (MoChA), which adaptively splits the input sequence into small chunks over which soft attention is computed. We show that models utilizing MoChA can be trained efficiently with standard backpropagation while allowing online and linear-time decoding at test time. When applied to online speech recognition, we obtain state-of-the-art results and match the performance of a model using an offline soft attention mechanism. In document summarization experiments where we do not expect monotonic alignments, we show significantly improved performance compared to a baseline monotonic attention-based model. ",
      categories: ["stat.ML", "stat", "cs", "cs.CL"],
      title: "Monotonic Chunkwise Attention",
      arxiv_id: "1712.05382",
      authors: ["Chung-Cheng Chiu", "Raffel Colin", "Chiu Chung-Cheng"],
      journal_ref: null,
      comments: "ICLR camera-ready version",
      update_date: "2018-02-26T00:00:00",
    },
    {
      doi: null,
      submitter: "Guoqiang Zhong",
      abstract:
        "  Recurrent Neural Network (RNN) has been successfully applied in many sequence learning problems. Such as handwriting recognition, image description, natural language processing and video motion analysis. After years of development, researchers have improved the internal structure of the RNN and introduced many variants. Among others, Gated Recurrent Unit (GRU) is one of the most widely used RNN model. However, GRU lacks the capability of adaptively paying attention to certain regions or locations, so that it may cause information redundancy or loss during leaning. In this paper, we propose a RNN model, called Recurrent Attention Unit (RAU), which seamlessly integrates the attention mechanism into the interior of GRU by adding an attention gate. The attention gate can enhance GRU's ability to remember long-term memory and help memory cells quickly discard unimportant content. RAU is capable of extracting information from the sequential data by adaptively selecting a sequence of regions or locations and pay more attention to the selected regions during learning. Extensive experiments on image classification, sentiment classification and language modeling show that RAU consistently outperforms GRU and other baseline methods. ",
      categories: ["stat.ML", "cs.LG", "cs", "cs.CL", "stat", "cs.NE"],
      title: "Recurrent Attention Unit",
      arxiv_id: "1810.12754",
      authors: ["Guoqiang Zhong", "Ling Xiao", "Yue Guohua", "Zhong Guoqiang"],
      journal_ref: null,
      comments: "",
      update_date: "2018-11-01T00:00:00",
    },
    {
      doi: null,
      submitter: "Idan Schwartz",
      abstract:
        "  Dialog is an effective way to exchange information, but subtle details and nuances are extremely important. While significant progress has paved a path to address visual dialog with algorithms, details and nuances remain a challenge. Attention mechanisms have demonstrated compelling results to extract details in visual question answering and also provide a convincing framework for visual dialog due to their interpretability and effectiveness. However, the many data utilities that accompany visual dialog challenge existing attention techniques. We address this issue and develop a general attention mechanism for visual dialog which operates on any number of data utilities. To this end, we design a factor graph based attention mechanism which combines any number of utility representations. We illustrate the applicability of the proposed approach on the challenging and recently introduced VisDial datasets, outperforming recent state-of-the-art methods by 1.1 MRR. Our ensemble model improved the MRR score on VisDial1.0 by more than 6 ",
      categories: ["cs.LG", "cs.CV", "cs.CL", "cs", "cs.AI", "cs.IR"],
      title: "Factor Graph Attention",
      arxiv_id: "1904.05880",
      authors: [
        "Yu Seunghak",
        "Schwartz Idan",
        "Hazan Tamir",
        "Idan Schwartz",
        "Schwing Alexander",
      ],
      journal_ref: null,
      comments:
        "Accepted to CVPR 2019; revised version includes bottom-up features",
      update_date: "2020-03-10T00:00:00",
    },
    {
      doi: null,
      submitter: "Sofia Serrano",
      abstract:
        "  Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components' representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components' overall importance to a model, it is by no means a fail-safe indicator. ",
      categories: ["cs", "cs.CL"],
      title: "Is Attention Interpretable?",
      arxiv_id: "1906.03731",
      authors: ["Serrano Sofia", "Smith Noah A.", "Sofia Serrano"],
      journal_ref: null,
      comments: "To appear at ACL 2019",
      update_date: "2019-06-11T00:00:00",
    },
    {
      doi: null,
      submitter: "Mat\\=iss Rikters",
      abstract:
        "  Attention distributions of the generated translations are a useful bi-product of attention-based recurrent neural network translation models and can be treated as soft alignments between the input and output tokens. In this work, we use attention distributions as a confidence metric for output translations. We present two strategies of using the attention distributions: filtering out bad translations from a large back-translated corpus, and selecting the best translation in a hybrid setup of two different translation systems. While manual evaluation indicated only a weak correlation between our confidence score and human judgments, the use-cases showed improvements of up to 2.22 BLEU points for filtering and 0.99 points for hybrid translation, tested on English<->German and English<->Latvian translation. ",
      categories: ["cs", "cs.CL"],
      title: "Confidence through Attention",
      arxiv_id: "1710.03743",
      authors: ["Fishel Mark", "Rikters Matƒ´ss", "Mat\\=iss Rikters"],
      journal_ref: null,
      comments: "",
      update_date: "2017-10-11T00:00:00",
    },
    {
      doi: null,
      submitter: '\\c{C}a\\u{g}lar G\\"ul\\c{c}ehre',
      abstract:
        "  We introduce hyperbolic attention networks to endow neural networks with enough capacity to match the complexity of data with hierarchical and power-law structure. A few recent approaches have successfully demonstrated the benefits of imposing hyperbolic geometry on the parameters of shallow networks. We extend this line of work by imposing hyperbolic geometry on the activations of neural networks. This allows us to exploit hyperbolic geometry to reason about embeddings produced by deep networks. We achieve this by re-expressing the ubiquitous mechanism of soft attention in terms of operations defined for hyperboloid and Klein models. Our method shows improvements in terms of generalization on neural machine translation, learning on graphs and visual question answering tasks while keeping the neural representations compact. ",
      categories: ["cs.NE", "cs"],
      title: "Hyperbolic Attention Networks",
      arxiv_id: "1805.09786",
      authors: [
        "Pascanu Razvan",
        "Razavi Ali",
        "Bapst Victor",
        "de Freitas Nando",
        "Raposo David",
        "Denil Misha",
        '\\c{C}a\\u{g}lar G\\"ul\\c{c}ehre',
        "Hermann Karl Moritz",
        "Santoro Adam",
        "Gulcehre Caglar",
        "Battaglia Peter",
        "Malinowski Mateusz",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2018-05-25T00:00:00",
    },
    {
      doi: null,
      submitter: "Jin-Hwa Kim",
      abstract:
        "  Attention networks in multimodal learning provide an efficient way to utilize given visual information selectively. However, the computational cost to learn attention distributions for every pair of multimodal input channels is prohibitively expensive. To solve this problem, co-attention builds two separate attention distributions for each modality neglecting the interaction between multimodal inputs. In this paper, we propose bilinear attention networks (BAN) that find bilinear attention distributions to utilize given vision-language information seamlessly. BAN considers bilinear interactions among two groups of input channels, while low-rank bilinear pooling extracts the joint representations for each pair of channels. Furthermore, we propose a variant of multimodal residual networks to exploit eight-attention maps of the BAN efficiently. We quantitatively and qualitatively evaluate our model on visual question answering (VQA 2.0) and Flickr30k Entities datasets, showing that BAN significantly outperforms previous methods and achieves new state-of-the-arts on both datasets. ",
      categories: ["cs.LG", "cs.CV", "cs.CL", "cs", "cs.AI"],
      title: "Bilinear Attention Networks",
      arxiv_id: "1805.07932",
      authors: [
        "Kim Jin-Hwa",
        "Jin-Hwa Kim",
        "Jun Jaehyun",
        "Zhang Byoung-Tak",
      ],
      journal_ref: null,
      comments: "Accepted by NIPS 2018; Figure 1 was updated",
      update_date: "2018-10-22T00:00:00",
    },
    {
      doi: null,
      submitter: "Eric J. Horvitz",
      abstract:
        "  We introduce utility-directed procedures for mediating the flow of potentially distracting alerts and communications to computer users. We present models and inference procedures that balance the context-sensitive costs of deferring alerts with the cost of interruption. We describe the challenge of reasoning about such costs under uncertainty via an analysis of user activity and the content of notifications. After introducing principles of attention-sensitive alerting, we focus on the problem of guiding alerts about email messages. We dwell on the problem of inferring the expected criticality of email and discuss work on the Priorities system, centering on prioritizing email by criticality and modulating the communication of notifications to users about the presence and nature of incoming email. ",
      categories: ["cs.HC", "cs", "cs.AI"],
      title: "Attention-Sensitive Alerting",
      arxiv_id: "1301.6707",
      authors: [
        "Eric J. Horvitz",
        "Horvitz Eric J.",
        "Jacobs Andy",
        "Hovel David",
      ],
      journal_ref: null,
      comments:
        "Appears in Proceedings of the Fifteenth Conference on Uncertainty in   Artificial Intelligence (UAI1999)",
      update_date: "2013-01-30T00:00:00",
    },
    {
      doi: null,
      submitter: "Barco You Mr.",
      abstract:
        "  Attention endows animals an ability to concentrate on the most relevant information among a deluge of distractors at any given time, either through volitionally 'top-down' biasing, or driven by automatically 'bottom-up' saliency of stimuli, in favour of advantageous competition in neural modulations for information processing. Nevertheless, instead of being limited to perceive simple features, human and other advanced animals adaptively learn the world into categories and abstract concepts from experiences, imparting the world meanings. This thesis suggests that the high-level cognitive ability of human is more likely driven by attention basing on abstract perceptions, which is defined as concept based attention (CbA). ",
      categories: ["q-bio.NC", "q-bio", "cs", "cs.AI"],
      title: "Concept based Attention",
      arxiv_id: "1605.03416",
      authors: ["Barco You Mr.", "Hub Matthias", "You Jie", "Yang Xin"],
      journal_ref: null,
      comments: "7 pages, 2 figures",
      update_date: "2016-05-13T00:00:00",
    },
    {
      doi: null,
      submitter: "Yoon Kim",
      abstract:
        "  Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention. ",
      categories: ["cs.NE", "cs.LG", "cs", "cs.CL"],
      title: "Structured Attention Networks",
      arxiv_id: "1702.00887",
      authors: [
        "Denton Carl",
        "Kim Yoon",
        "Hoang Luong",
        "Yoon Kim",
        "Rush Alexander M.",
      ],
      journal_ref: null,
      comments: "ICLR 2017",
      update_date: "2017-02-17T00:00:00",
    },
    {
      doi: null,
      submitter: "Juan Pino",
      abstract:
        "  Simultaneous machine translation models start generating a target sequence before they have encoded or read the source sequence. Recent approaches for this task either apply a fixed policy on a state-of-the art Transformer model, or a learnable monotonic attention on a weaker recurrent neural network-based structure. In this paper, we propose a new attention mechanism, Monotonic Multihead Attention (MMA), which extends the monotonic attention mechanism to multihead attention. We also introduce two novel and interpretable approaches for latency control that are specifically designed for multiple attentions heads. We apply MMA to the simultaneous machine translation task and demonstrate better latency-quality tradeoffs compared to MILk, the previous state-of-the-art approach. We also analyze how the latency controls affect the attention span and we motivate the introduction of our model by analyzing the effect of the number of decoder layers and heads on quality and latency. ",
      categories: ["cs", "cs.CL"],
      title: "Monotonic Multihead Attention",
      arxiv_id: "1909.12406",
      authors: [
        "Gu Jiatao",
        "Puzon Liezl",
        "Pino Juan",
        "Ma Xutai",
        "Cross James",
        "Juan Pino",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2019-09-30T00:00:00",
    },
    {
      doi: null,
      submitter: "Shawn Tan",
      abstract:
        "  The self-attention mechanism traditionally relies on the softmax operator, necessitating positional embeddings like RoPE, or position biases to account for token order. But current methods using still face length generalisation challenges. We propose an alternative attention mechanism based on the stick-breaking process: For each token before the current, we determine a break point Œ≤_i,j, which represents the proportion of the remaining stick to allocate to the current token. We repeat the process until the stick is fully allocated, resulting in a sequence of attention weights. This process naturally incorporates recency bias, which has linguistic motivations for grammar parsing (Shen et. al., 2017). We study the implications of replacing the conventional softmax-based attention mechanism with stick-breaking attention. We then discuss implementation of numerically stable stick-breaking attention and adapt Flash Attention to accommodate this mechanism. When used as a drop-in replacement for current softmax+RoPE attention systems, we find that stick-breaking attention performs competitively with current methods on length generalisation and downstream tasks. Stick-breaking also performs well at length generalisation, allowing a model trained with 2^11 context window to perform well at 2^14 with perplexity improvements. ",
      categories: ["cs.CL", "cs.LG", "cs", "cs.AI"],
      title: "Stick-breaking Attention",
      arxiv_id: "2410.17980",
      authors: [
        "Shen Yikang",
        "Yang Songlin",
        "Courville Aaron",
        "Panda Rameswar",
        "Tan Shawn",
        "Shawn Tan",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2024-10-28T00:00:00",
    },
    {
      doi: null,
      submitter: "Hao Peng",
      abstract:
        "  Transformers are state-of-the-art models for a variety of sequence modeling tasks. At their core is an attention function which models pairwise interactions between the inputs at every timestep. While attention is powerful, it does not scale efficiently to long sequences due to its quadratic time and space complexity in the sequence length. We propose RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, and explore its application in transformers. RFA can be used as a drop-in replacement for conventional softmax attention and offers a straightforward way of learning with recency bias through an optional gating mechanism. Experiments on language modeling and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines. In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer. Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets. Our analysis shows that RFA's efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints. ",
      categories: ["cs", "cs.CL"],
      title: "Random Feature Attention",
      arxiv_id: "2103.02143",
      authors: [
        "Schwartz Roy",
        "Pappas Nikolaos",
        "Yogatama Dani",
        "Hao Peng",
        "Peng Hao",
        "Smith Noah A.",
        "Kong Lingpeng",
      ],
      journal_ref: null,
      comments: "ICLR 2021",
      update_date: "2021-03-23T00:00:00",
    },
    {
      doi: null,
      submitter: "Lorenzo Giusti",
      abstract:
        "  Since their introduction, graph attention networks achieved outstanding results in graph representation learning tasks. However, these networks consider only pairwise relationships among nodes and then they are not able to fully exploit higher-order interactions present in many real world data-sets. In this paper, we introduce Cell Attention Networks (CANs), a neural architecture operating on data defined over the vertices of a graph, representing the graph as the 1-skeleton of a cell complex introduced to capture higher order interactions. In particular, we exploit the lower and upper neighborhoods, as encoded in the cell complex, to design two independent masked self-attention mechanisms, thus generalizing the conventional graph attention strategy. The approach used in CANs is hierarchical and it incorporates the following steps: i) a lifting algorithm that learns edge features from node features; ii) a cell attention mechanism to find the optimal combination of edge features over both lower and upper neighbors; iii) a hierarchical edge pooling mechanism to extract a compact meaningful set of features. The experimental results show that CAN is a low complexity strategy that compares favorably with state of the art results on graph-based learning tasks. ",
      categories: ["cs.LG", "cs", "cs.AI"],
      title: "Cell Attention Networks",
      arxiv_id: "2209.08179",
      authors: [
        "Battiloro Claudio",
        "Sardellitti Stefania",
        "Di Lorenzo Paolo",
        "Lorenzo Giusti",
        "Barbarossa Sergio",
        "Testa Lucia",
        "Giusti Lorenzo",
      ],
      journal_ref: null,
      comments: "Preprint, under review",
      update_date: "2022-09-20T00:00:00",
    },
    {
      doi: null,
      submitter: "Leo Feng",
      abstract:
        "  Cross Attention is a popular method for retrieving information from a set of context tokens for making predictions. At inference time, for each prediction, Cross Attention scans the full set of ùí™(N) tokens. In practice, however, often only a small subset of tokens are required for good performance. Methods such as Perceiver IO are cheap at inference as they distill the information to a smaller-sized set of latent tokens L < N on which cross attention is then applied, resulting in only ùí™(L) complexity. However, in practice, as the number of input tokens and the amount of information to distill increases, the number of latent tokens needed also increases significantly. In this work, we propose Tree Cross Attention (TCA) - a module based on Cross Attention that only retrieves information from a logarithmic ùí™(log(N)) number of tokens for performing inference. TCA organizes the data in a tree structure and performs a tree search at inference time to retrieve the relevant tokens for prediction. Leveraging TCA, we introduce ReTreever, a flexible architecture for token-efficient inference. We show empirically that Tree Cross Attention (TCA) performs comparable to Cross Attention across various classification and uncertainty regression tasks while being significantly more token-efficient. Furthermore, we compare ReTreever against Perceiver IO, showing significant gains while using the same number of tokens for inference. ",
      categories: ["cs.LG", "cs"],
      title: "Tree Cross Attention",
      arxiv_id: "2309.17388",
      authors: [
        "Feng Leo",
        "Leo Feng",
        "Tung Frederick",
        "Ahmed Mohamed Osama",
        "Bengio Yoshua",
        "Hajimirsadeghi Hossein",
      ],
      journal_ref: null,
      comments: "Accepted by ICLR 2024",
      update_date: "2024-03-04T00:00:00",
    },
    {
      doi: null,
      submitter: "Kevin Christian Wibisono",
      abstract:
        "  The self-attention mechanism is the backbone of the transformer neural network underlying most large language models. It can capture complex word patterns and long-range dependencies in natural language. This paper introduces exponential family attention (EFA), a probabilistic generative model that extends self-attention to handle high-dimensional sequence, spatial, or spatial-temporal data of mixed data types, including both discrete and continuous observations. The key idea of EFA is to model each observation conditional on all other existing observations, called the context, whose relevance is learned in a data-driven way via an attention-based latent factor model. In particular, unlike static latent embeddings, EFA uses the self-attention mechanism to capture dynamic interactions in the context, where the relevance of each context observations depends on other observations. We establish an identifiability result and provide a generalization guarantee on excess loss for EFA. Across real-world and synthetic data sets ‚Äì including U.S. city temperatures, Instacart shopping baskets, and MovieLens ratings ‚Äì we find that EFA consistently outperforms existing models in capturing complex latent structures and reconstructing held-out data. ",
      categories: ["stat.ML", "cs.LG", "cs", "stat"],
      title: "Exponential Family Attention",
      arxiv_id: "2501.16790",
      authors: [
        "Kevin Christian Wibisono",
        "Wibisono Kevin Christian",
        "Wang Yixin",
      ],
      journal_ref: null,
      comments: "47 pages",
      update_date: "2025-01-29T00:00:00",
    },
    {
      doi: null,
      submitter: "Difan Deng",
      abstract:
        "  We present Neural Attention Search (NAtS), a framework that automatically evaluates the importance of each token within a sequence and determines if the corresponding token can be dropped after several steps. This approach can efficiently reduce the KV cache sizes required by transformer-based models during inference and thus reduce inference costs. In this paper, we design a search space that contains three token types: (i) Global Tokens will be preserved and queried by all the following tokens. (ii) Local Tokens survive until the next global token appears. (iii) Sliding Window Tokens have an impact on the inference of a fixed size of the next following tokens. Similar to the One-Shot Neural Architecture Search approach, this token-type information can be learned jointly with the architecture weights via a learnable attention mask. Experiments on both training a new transformer from scratch and fine-tuning existing large language models show that NAtS can efficiently reduce the KV cache size required for the models while maintaining the models' performance. ",
      categories: ["cs.AI", "cs", "cs.CL"],
      title: "Neural Attention Search",
      arxiv_id: "2502.13251",
      authors: ["Lindauer Marius", "Deng Difan", "Difan Deng"],
      journal_ref: null,
      comments: "18 pages, 8 figures",
      update_date: "2025-02-21T00:00:00",
    },
    {
      doi: null,
      submitter: "Kai-Fu Yang",
      abstract:
        "  Visual attention plays a critical role when our visual system executes active visual tasks by interacting with the physical scene. However, how to encode the visual object relationship in the psychological world of our brain deserves to be explored. In the field of computer vision, predicting visual fixations or scanpaths is a usual way to explore the visual attention and behaviors of human observers when viewing a scene. Most existing methods encode visual attention using individual fixations or scanpaths based on the raw gaze shift data collected from human observers. This may not capture the common attention pattern well, because without considering the semantic information of the viewed scene, raw gaze shift data alone contain high inter- and intra-observer variability. To address this issue, we propose a new attention representation, called Attention Graph, to simultaneously code the visual saliency and scanpath in a graph-based representation and better reveal the common attention behavior of human observers. In the attention graph, the semantic-based scanpath is defined by the path on the graph, while saliency of objects can be obtained by computing fixation density on each node. Systemic experiments demonstrate that the proposed attention graph combined with our new evaluation metrics provides a better benchmark for evaluating attention prediction methods. Meanwhile, extra experiments demonstrate the promising potentials of the proposed attention graph in assessing human cognitive states, such as autism spectrum disorder screening and age classification. ",
      categories: ["cs.CV", "q-bio.QM", "q-bio", "cs"],
      title: "Visual Attention Graph",
      arxiv_id: "2503.08531",
      authors: ["Yang Kai-Fu", "Kai-Fu Yang", "Li Yong-Jie"],
      journal_ref: null,
      comments: "20 pages, 14 figures",
      update_date: "2025-03-12T00:00:00",
    },
    {
      doi: "10.1016/j.physa.2003.10.081",
      submitter: "Joseph O. Indekeu",
      abstract:
        '  In this Note a social network model for opinion formation is proposed in which a person connected to q partners pays an attention 1/q to each partner. The mutual attention between two connected persons i and j is taken equal to the geometric mean 1/‚àö(q_iq_j). Opinion is represented as usual by an Ising spin s=¬± 1 and mutual attention is given through a two-spin coupling J_ij = J Q/‚àö(q_iq_j), Q being the average connectivity in the network. Connectivity diminishes attention and only persons with low connectivity can pay special attention to each other leading to a durable common (or opposing) opinion. The model is solved in "mean-field" approximation and a critical "temperature" T_c proportional to JQ is found, which is independent of the number of persons N, for large N. ',
      categories: ["physics.soc-ph", "physics"],
      title: "Special Attention Network",
      arxiv_id: "physics/0310113",
      authors: ["Joseph O. Indekeu", "Indekeu J. O."],
      journal_ref: null,
      comments: "5 pages, no figures",
      update_date: "2009-11-10T00:00:00",
    },
    {
      doi: null,
      submitter: "Jinyi Wu",
      abstract:
        "  We propose Self-Supervised Implicit Attention (SSIA), a new approach that adaptively guides deep neural network models to gain attention by exploiting the properties of the models themselves. SSIA is a novel attention mechanism that does not require any extra parameters, computation, or memory access costs during inference, which is in contrast to existing attention mechanism. In short, by considering attention weights as higher-level semantic information, we reconsidered the implementation of existing attention mechanisms and further propose generating supervisory signals from higher network layers to guide lower network layers for parameter updates. We achieved this by building a self-supervised learning task using the hierarchical features of the network itself, which only works at the training stage. To verify the effectiveness of SSIA, we performed a particular implementation (called an SSIA block) in convolutional neural network models and validated it on several image classification datasets. The experimental results show that an SSIA block can significantly improve the model performance, even outperforms many popular attention methods that require additional parameters and computation costs, such as Squeeze-and-Excitation and Convolutional Block Attention Module. Our implementation will be available on GitHub. ",
      categories: ["cs.CV", "cs"],
      title:
        "Self-Supervised Implicit Attention: Guided Attention by The Model Itself",
      arxiv_id: "2206.07434",
      authors: ["Wu Jinyi", "Jinyi Wu", "Gong Xun", "Zhang Zhemin"],
      journal_ref: null,
      comments: "",
      update_date: "2022-07-22T00:00:00",
    },
    {
      doi: null,
      submitter: "Paria Mehrani",
      abstract:
        "  Recently, a considerable number of studies in computer vision involves deep neural architectures called vision transformers. Visual processing in these models incorporates computational models that are claimed to implement attention mechanisms. Despite an increasing body of work that attempts to understand the role of attention mechanisms in vision transformers, their effect is largely unknown. Here, we asked if the attention mechanisms in vision transformers exhibit similar effects as those known in human visual attention. To answer this question, we revisited the attention formulation in these models and found that despite the name, computationally, these models perform a special class of relaxation labeling with similarity grouping effects. Additionally, whereas modern experimental findings reveal that human visual attention involves both feed-forward and feedback mechanisms, the purely feed-forward architecture of vision transformers suggests that attention in these models will not have the same effects as those known in humans. To quantify these observations, we evaluated grouping performance in a family of vision transformers. Our results suggest that self-attention modules group figures in the stimuli based on similarity in visual features such as color. Also, in a singleton detection experiment as an instance of saliency detection, we studied if these models exhibit similar effects as those of feed-forward visual salience mechanisms utilized in human visual attention. We found that generally, the transformer-based attention modules assign more salience either to distractors or the ground. Together, our study suggests that the attention mechanisms in vision transformers perform similarity grouping and not attention. ",
      categories: ["cs.CV", "cs"],
      title:
        "Self-attention in Vision Transformers Performs Perceptual Grouping, Not   Attention",
      arxiv_id: "2303.01542",
      authors: ["Paria Mehrani", "Mehrani Paria", "Tsotsos John K."],
      journal_ref: null,
      comments: "",
      update_date: "2023-03-06T00:00:00",
    },
    {
      doi: null,
      submitter: "Matteo Pagliardini",
      abstract:
        "  Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention ‚Äì which is the only component scaling quadratically w.r.t. the sequence length ‚Äì becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by 2.0√ó and 3.3√ó for sequences of respectively 8k and 16k tokens. ",
      categories: ["cs.CL", "cs.LG", "cs", "cs.AI"],
      title:
        "Faster Causal Attention Over Large Sequences Through Sparse Flash   Attention",
      arxiv_id: "2306.01160",
      authors: [
        "Jaggi Martin",
        "Paliotta Daniele",
        "Pagliardini Matteo",
        "Fleuret Fran√ßois",
        "Matteo Pagliardini",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2023-06-05T00:00:00",
    },
    {
      doi: null,
      submitter: "Dongchen Han",
      abstract:
        "  The attention module is the key component in Transformers. While the global attention mechanism offers high expressiveness, its excessive computational cost restricts its applicability in various scenarios. In this paper, we propose a novel attention paradigm, Agent Attention, to strike a favorable balance between computational efficiency and representation power. Specifically, the Agent Attention, denoted as a quadruple (Q, A, K, V), introduces an additional set of agent tokens A into the conventional attention module. The agent tokens first act as the agent for the query tokens Q to aggregate information from K and V, and then broadcast the information back to Q. Given the number of agent tokens can be designed to be much smaller than the number of query tokens, the agent attention is significantly more efficient than the widely adopted Softmax attention, while preserving global context modelling capability. Interestingly, we show that the proposed agent attention is equivalent to a generalized form of linear attention. Therefore, agent attention seamlessly integrates the powerful Softmax attention and the highly efficient linear attention. Extensive experiments demonstrate the effectiveness of agent attention with various vision Transformers and across diverse vision tasks, including image classification, object detection, semantic segmentation and image generation. Notably, agent attention has shown remarkable performance in high-resolution scenarios, owning to its linear attention nature. For instance, when applied to Stable Diffusion, our agent attention accelerates generation and substantially enhances image generation quality without any additional training. Code is available at https://github.com/LeapLabTHU/Agent-Attention. ",
      categories: ["cs.CV", "cs"],
      title:
        "Agent Attention: On the Integration of Softmax and Linear Attention",
      arxiv_id: "2312.08874",
      authors: [
        "Ye Tianzhu",
        "Song Shiji",
        "Xia Zhuofan",
        "Pan Siyuan",
        "Han Yizeng",
        "Huang Gao",
        "Dongchen Han",
        "Wan Pengfei",
        "Han Dongchen",
      ],
      journal_ref: null,
      comments: "ECCV 2024",
      update_date: "2024-07-16T00:00:00",
    },
    {
      doi: null,
      submitter: "Yanbin Hao",
      abstract:
        "  Attention mechanisms have significantly boosted the performance of video classification neural networks thanks to the utilization of perspective contexts. However, the current research on video attention generally focuses on adopting a specific aspect of contexts (e.g., channel, spatial/temporal, or global context) to refine the features and neglects their underlying correlation when computing attentions. This leads to incomplete context utilization and hence bears the weakness of limited performance improvement. To tackle the problem, this paper proposes an efficient attention-in-attention (AIA) method for element-wise feature refinement, which investigates the feasibility of inserting the channel context into the spatio-temporal attention learning module, referred to as CinST, and also its reverse variant, referred to as STinC. Specifically, we instantiate the video feature contexts as dynamics aggregated along a specific axis with global average and max pooling operations. The workflow of an AIA module is that the first attention block uses one kind of context information to guide the gating weights calculation of the second attention that targets at the other context. Moreover, all the computational operations in attention units act on the pooled dimension, which results in quite few computational cost increase (<0.02%). To verify our method, we densely integrate it into two classical video network backbones and conduct extensive experiments on several standard video classification benchmarks. The source code of our AIA is available at <https://github.com/haoyanbin918/Attention-in-Attention>. ",
      categories: ["cs.CV", "cs"],
      title:
        "Attention in Attention: Modeling Context Correlation for Efficient Video   Classification",
      arxiv_id: "2204.09303",
      authors: [
        "He Xiangnan",
        "Xu Tong",
        "Wang Shuo",
        "Gao Xinjian",
        "Wu Jinmeng",
        "Yanbin Hao",
        "Cao Pei",
        "Hao Yanbin",
      ],
      journal_ref: null,
      comments: "13 pages",
      update_date: "2022-04-21T00:00:00",
    },
    {
      doi: null,
      submitter: "Erwin Daniel Lopez Zapata",
      abstract:
        "  This paper proposes Attention-Seeker, an unsupervised keyphrase extraction method that leverages self-attention maps from a Large Language Model to estimate the importance of candidate phrases. Our approach identifies specific components - such as layers, heads, and attention vectors - where the model pays significant attention to the key topics of the text. The attention weights provided by these components are then used to score the candidate phrases. Unlike previous models that require manual tuning of parameters (e.g., selection of heads, prompts, hyperparameters), Attention-Seeker dynamically adapts to the input text without any manual adjustments, enhancing its practical applicability. We evaluate Attention-Seeker on four publicly available datasets: Inspec, SemEval2010, SemEval2017, and Krapivin. Our results demonstrate that, even without parameter tuning, Attention-Seeker outperforms most baseline models, achieving state-of-the-art performance on three out of four datasets, particularly excelling in extracting keyphrases from long documents. ",
      categories: ["cs.IR", "cs", "cs.CL"],
      title:
        "Attention-Seeker: Dynamic Self-Attention Scoring for Unsupervised   Keyphrase Extraction",
      arxiv_id: "2409.10907",
      authors: [
        "Z. Erwin D. L√≥pez",
        "Shimada Atsushi",
        "Tang Cheng",
        "Erwin Daniel Lopez Zapata",
      ],
      journal_ref: null,
      comments:
        "This version has been accepted for presentation at COLING 2025, and   all peer-reviewed changes have been incorporated",
      update_date: "2024-12-17T00:00:00",
    },
    {
      doi: null,
      submitter: "Joseph DeRose",
      abstract:
        "  Advances in language modeling have led to the development of deep attention-based models that are performant across a wide variety of natural language processing (NLP) problems. These language models are typified by a pre-training process on large unlabeled text corpora and subsequently fine-tuned for specific tasks. Although considerable work has been devoted to understanding the attention mechanisms of pre-trained models, it is less understood how a model's attention mechanisms change when trained for a target NLP task. In this paper, we propose a visual analytics approach to understanding fine-tuning in attention-based language models. Our visualization, Attention Flows, is designed to support users in querying, tracing, and comparing attention within layers, across layers, and amongst attention heads in Transformer-based language models. To help users gain insight on how a classification decision is made, our design is centered on depicting classification-based attention at the deepest layer and how attention from prior layers flows throughout words in the input. Attention Flows supports the analysis of a single model, as well as the visual comparison between pre-trained and fine-tuned models via their similarities and differences. We use Attention Flows to study attention mechanisms in various sentence understanding tasks and highlight how attention evolves to address the nuances of solving these tasks. ",
      categories: ["cs.HC", "cs", "cs.CL"],
      title:
        "Attention Flows: Analyzing and Comparing Attention Mechanisms in   Language Models",
      arxiv_id: "2009.07053",
      authors: [
        "Berger Matthew",
        "DeRose Joseph F",
        "Wang Jiayao",
        "Joseph DeRose",
      ],
      journal_ref: null,
      comments:
        "11 pages, 12 figures, to be published in IEEE Transactions on   Visualization and Computer Graphics",
      update_date: "2020-09-16T00:00:00",
    },
    {
      doi: null,
      submitter: "Wei He",
      abstract:
        "  Recently, many plug-and-play self-attention modules are proposed to enhance the model generalization by exploiting the internal information of deep convolutional neural networks (CNNs). Previous works lay an emphasis on the design of attention module for specific functionality, e.g., light-weighted or task-oriented attention. However, they ignore the importance of where to plug in the attention module since they connect the modules individually with each block of the entire CNN backbone for granted, leading to incremental computational cost and number of parameters with the growth of network depth. Thus, we propose a framework called Efficient Attention Network (EAN) to improve the efficiency for the existing attention modules. In EAN, we leverage the sharing mechanism (Huang et al. 2020) to share the attention module within the backbone and search where to connect the shared attention module via reinforcement learning. Finally, we obtain the attention network with sparse connections between the backbone and modules, while (1) maintaining accuracy (2) reducing extra parameter increment and (3) accelerating inference. Extensive experiments on widely-used benchmarks and popular attention networks show the effectiveness of EAN. Furthermore, we empirically illustrate that our EAN has the capacity of transferring to other tasks and capturing the informative features. The code is available at https://github.com/gbup-group/EAN-efficient-attention-network. ",
      categories: ["cs.CV", "cs.LG", "cs", "cs.AI"],
      title:
        "Efficient Attention Network: Accelerate Attention by Searching Where to   Plug",
      arxiv_id: "2011.14058",
      authors: [
        "Huang Zhongzhan",
        "Liang Mingfu",
        "Yang Haizhao",
        "Wei He",
        "He Wei",
        "Liang Senwei",
      ],
      journal_ref: null,
      comments: "",
      update_date: "2021-07-13T00:00:00",
    },
    {
      doi: null,
      submitter: "Tom Kersten",
      abstract:
        "  The field of explainable AI has recently seen an explosion in the number of explanation methods for highly non-linear deep neural networks. The extent to which such methods ‚Äì that are often proposed and tested in the domain of computer vision ‚Äì are appropriate to address the explainability challenges in NLP is yet relatively unexplored. In this work, we consider Contextual Decomposition (CD) ‚Äì a Shapley-based input feature attribution method that has been shown to work well for recurrent NLP models ‚Äì and we test the extent to which it is useful for models that contain attention operations. To this end, we extend CD to cover the operations necessary for attention-based models. We then compare how long distance subject-verb relationships are processed by models with and without attention, considering a number of different syntactic structures in two different languages: English and Dutch. Our experiments confirm that CD can successfully be applied for attention-based models as well, providing an alternative Shapley-based attribution method for modern neural networks. In particular, using CD, we show that the English and Dutch models demonstrate similar processing behaviour, but that under the hood there are consistent differences between our attention and non-attention models. ",
      categories: ["cs", "cs.CL"],
      title:
        "Attention vs non-attention for a Shapley-based explanation method",
      arxiv_id: "2104.12424",
      authors: [
        "Kersten Tom",
        "Hupkes Dieuwke",
        "Jumelet Jaap",
        "Tom Kersten",
        "Wong Hugh Mee",
      ],
      journal_ref: null,
      comments: "Accepted for publication at DeeLIO 2021",
      update_date: "2021-04-27T00:00:00",
    },
    {
      doi: "10.1109/ACCESS.2021.3093456",
      submitter: "Shunsuke Kitada",
      abstract:
        "  Although attention mechanisms have been applied to a variety of deep learning models and have been shown to improve the prediction performance, it has been reported to be vulnerable to perturbations to the mechanism. To overcome the vulnerability to perturbations in the mechanism, we are inspired by adversarial training (AT), which is a powerful regularization technique for enhancing the robustness of the models. In this paper, we propose a general training technique for natural language processing tasks, including AT for attention (Attention AT) and more interpretable AT for attention (Attention iAT). The proposed techniques improved the prediction performance and the model interpretability by exploiting the mechanisms with AT. In particular, Attention iAT boosts those advantages by introducing adversarial perturbation, which enhances the difference in the attention of the sentences. Evaluation experiments with ten open datasets revealed that AT for attention mechanisms, especially Attention iAT, demonstrated (1) the best performance in nine out of ten tasks and (2) more interpretable attention (i.e., the resulting attention correlated more strongly with gradient-based word importance) for all tasks. Additionally, the proposed techniques are (3) much less dependent on perturbation size in AT. Our code is available at https://github.com/shunk031/attention-meets-perturbation ",
      categories: ["cs.AI", "cs.LG", "cs", "cs.CL"],
      title:
        "Attention Meets Perturbations: Robust and Interpretable Attention with   Adversarial Training",
      arxiv_id: "2009.12064",
      authors: ["Shunsuke Kitada", "Iyatomi Hitoshi", "Kitada Shunsuke"],
      journal_ref: null,
      comments: "12 pages, 4 figures. Accepted by IEEE Access on Jun. 21, 2021",
      update_date: "2022-11-23T00:00:00",
    },
  ],
  available_facets: [
    {
      field: "categories",
      value: "cs",
      count: 49679,
    },
    {
      field: "categories",
      value: "cs.CV",
      count: 19698,
    },
    {
      field: "categories",
      value: "cs.LG",
      count: 16219,
    },
    {
      field: "categories",
      value: "cs.AI",
      count: 10500,
    },
    {
      field: "categories",
      value: "cs.CL",
      count: 8569,
    },
    {
      field: "categories",
      value: "eess",
      count: 7364,
    },
    {
      field: "categories",
      value: "math",
      count: 6925,
    },
    {
      field: "categories",
      value: "cond-mat",
      count: 5990,
    },
    {
      field: "categories",
      value: "physics",
      count: 5004,
    },
    {
      field: "categories",
      value: "stat",
      count: 4735,
    },
    {
      field: "authors",
      value: "Liu Yang",
      count: 206,
    },
    {
      field: "authors",
      value: "Wang Wei",
      count: 135,
    },
    {
      field: "authors",
      value: "Tao Dacheng",
      count: 132,
    },
    {
      field: "authors",
      value: "Zhang Lei",
      count: 122,
    },
    {
      field: "authors",
      value: "Zhang Wei",
      count: 116,
    },
    {
      field: "authors",
      value: "Zhang Yu",
      count: 108,
    },
    {
      field: "authors",
      value: "Chen Hao",
      count: 107,
    },
    {
      field: "authors",
      value: "Li Yang",
      count: 105,
    },
    {
      field: "authors",
      value: "Li Xiang",
      count: 100,
    },
    {
      field: "authors",
      value: "Wang Xin",
      count: 100,
    },
  ],
};
