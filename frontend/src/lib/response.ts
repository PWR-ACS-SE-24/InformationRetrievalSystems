export const response = {
  pagination: {
    total_records: 2146198,
    total_pages: 71540,
    current_page: 0,
    size: 30,
  },
  time_to_search: 5826,
  papers: [
    {
      doi: null,
      submitter: "Llion Jones",
      abstract:
        "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
      categories: ["cs.CL", "cs", "cs.LG"],
      update_date: "2023-08-02T00:00:00",
      title: "Attention Is All You Need",
      arxiv_id: "1706.03762",
      authors: [
        "Kaiser Lukasz",
        "Vaswani Ashish",
        "Polosukhin Illia",
        "Uszkoreit Jakob",
        "Parmar Niki",
        "Jones Llion",
        "Gomez Aidan N.",
        "Llion Jones",
        "Shazeer Noam",
      ],
      journal_ref: null,
      comments: "15 pages, 5 figures",
      create_date: "2017-06-12T00:00:00",
    },
    {
      doi: null,
      submitter: "Muhsin Murat Yaslioglu",
      abstract:
        "This work introduces a novel Retention Layer mechanism for Transformer based architectures, addressing their inherent lack of intrinsic retention capabilities. Unlike human cognition, which can encode and dynamically recall symbolic templates, Generative Pretrained Transformers rely solely on fixed pretrained weights and ephemeral context windows, limiting their adaptability. The proposed Retention Layer incorporates a persistent memory module capable of real time data population, dynamic recall, and guided output generation. This enhancement allows models to store, update, and reuse observed patterns across sessions, enabling incremental learning and bridging the gap between static pretraining and dynamic, context sensitive adaptation. The Retention Layer design parallels social learning processes, encompassing attention, retention, reproduction, and motivation stages. Technically, it integrates a memory attention mechanism and episodic buffers to manage memory scalability, mitigate overfitting, and ensure efficient recall. Applications span adaptive personal assistants, real time fraud detection, autonomous robotics, content moderation, and healthcare diagnostics. In each domain, the retention mechanism enables systems to learn incrementally, personalize outputs, and respond to evolving real world challenges effectively. By emulating key aspects of human learning, this retention enhanced architecture fosters a more fluid and responsive AI paradigm, paving the way for dynamic, session aware models that extend the capabilities of traditional Transformers into domains requiring continual adaptation.",
      categories: ["cs.AI", "cs", "cs.LG"],
      update_date: "2025-01-15T00:00:00",
      title: "Attention is All You Need Until You Need Retention",
      arxiv_id: "2501.09166",
      authors: ["Yaslioglu M. Murat", "Muhsin Murat Yaslioglu"],
      journal_ref: null,
      comments: "",
      create_date: "2025-01-15T00:00:00",
    },
    {
      doi: null,
      submitter: "Hongqiu Wu",
      abstract:
        "Beyond the success story of pre-trained language models (PrLMs) in recent natural language processing, they are susceptible to over-fitting due to unusual large model size. To this end, dropout serves as a therapy. However, existing methods like random-based, knowledge-based and search-based dropout are more general but less effective onto self-attention based models, which are broadly chosen as the fundamental architecture of PrLMs. In this paper, we propose a novel dropout method named AttendOut to let self-attention empowered PrLMs capable of more robust task-specific tuning. We demonstrate that state-of-the-art models with elaborate training design may achieve much stronger results. We verify the universality of our approach on extensive natural language processing tasks.",
      categories: ["cs.CL", "cs"],
      update_date: "2021-06-01T00:00:00",
      title: "Not All Attention Is All You Need",
      arxiv_id: "2104.04692",
      authors: ["Wu Hongqiu", "Hongqiu Wu", "Zhang Min", "Zhao Hai"],
      journal_ref: null,
      comments: "",
      create_date: "2021-04-10T00:00:00",
    },
    {
      doi: null,
      submitter: "Zhe Chen",
      abstract:
        "In recent years, the popular Transformer architecture has achieved great success in many application areas, including natural language processing and computer vision. Many existing works aim to reduce the computational and memory complexity of the self-attention mechanism in the Transformer by trading off performance. However, performance is key for the continuing success of the Transformer. In this paper, a family of drop-in replacements for the self-attention mechanism in the Transformer, called the Extractors, is proposed. Four types of the Extractors, namely the super high-performance Extractor (SHE), the higher-performance Extractor (HE), the worthwhile Extractor (WE), and the minimalist Extractor (ME), are proposed as examples. Experimental results show that replacing the self-attention mechanism with the SHE evidently improves the performance of the Transformer, whereas the simplified versions of the SHE, i.e., the HE, the WE, and the ME, perform close to or better than the self-attention mechanism with less computational and memory complexity. Furthermore, the proposed Extractors have the potential or are able to run faster than the self-attention mechanism since their critical paths of computation are much shorter. Additionally, the sequence prediction problem in the context of text generation is formulated using variable-length discrete-time Markov chains, and the Transformer is reviewed based on our understanding.",
      categories: ["cs.CL", "cs", "cs.LG", "cs.NE"],
      update_date: "2023-09-19T00:00:00",
      title: "Attention Is Not All You Need Anymore",
      arxiv_id: "2308.07661",
      authors: ["Zhe Chen", "Chen Zhe"],
      journal_ref: null,
      comments: "",
      create_date: "2023-08-15T00:00:00",
    },
    {
      doi: null,
      submitter: "Ryan Singh",
      abstract:
        "Attention mechanisms are a central property of cognitive systems allowing them to selectively deploy cognitive resources in a flexible manner. Attention has been long studied in the neurosciences and there are numerous phenomenological models that try to capture its core properties. Recently attentional mechanisms have become a dominating architectural choice of machine learning and are the central innovation of Transformers. The dominant intuition and formalism underlying their development has drawn on ideas of keys and queries in database management systems. In this work, we propose an alternative Bayesian foundation for attentional mechanisms and show how this unifies different attentional architectures in machine learning. This formulation allows to to identify commonality across different attention ML architectures as well as suggest a bridge to those developed in neuroscience. We hope this work will guide more sophisticated intuitions into the key properties of attention architectures and suggest new ones.",
      categories: ["cs.AI", "cs", "cs.LG", "cs.NE"],
      update_date: "2023-04-07T00:00:00",
      title: "Attention: Marginal Probability is All You Need?",
      arxiv_id: "2304.04556",
      authors: ["Buckley Christopher L.", "Ryan Singh", "Singh Ryan"],
      journal_ref: null,
      comments: "",
      create_date: "2023-04-07T00:00:00",
    },
    {
      doi: null,
      submitter: "Guoxin Feng",
      abstract:
        "The self-attention (SA) mechanism has demonstrated superior performance across various domains, yet it suffers from substantial complexity during both training and inference. The next-generation architecture, aiming at retaining the competitive performance of SA while achieving low-cost inference and efficient long-sequence training, primarily focuses on three approaches: linear attention, linear RNNs, and state space models. Although these approaches achieve reduced complexity than SA, they all have built-in performance degradation factors, such as diminished √¢¬Ä¬úspikiness√¢¬Ä¬ù and compression of historical information. In contrast to these approaches, we propose a novel element-wise attention mechanism, which uses the element-wise squared Euclidean distance, instead of the dot product operation, to compute similarity and approximates the quadratic complexity term exp(q_ick_jc) with a Taylor polynomial. This design achieves remarkable efficiency: during training, the element-wise attention has a complexity of ùí™(tLD), making long-sequence training both computationally and memory efficient, where L is the sequence length, D is the feature dimension, and t is the highest order of the polynomial; during inference, it can be reformulated as recurrent neural networks, achieving a inference complexity of ùí™(tD). Furthermore, the element-wise attention circumvents the performance degradation factors present in these approaches and achieves performance comparable to SA in both causal and non-causal forms.",
      categories: ["cs.AI", "cs", "cs.LG"],
      update_date: "2025-01-10T00:00:00",
      title: "Element-wise Attention Is All You Need",
      arxiv_id: "2501.05730",
      authors: ["Guoxin Feng", "Feng Guoxin"],
      journal_ref: null,
      comments: "",
      create_date: "2025-01-10T00:00:00",
    },
    {
      doi: null,
      submitter: "Yifeng Liu",
      abstract:
        "Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, significantly shrinking KV cache size at inference time. By factorizing these representations into contextual low-rank components (contextual factorization) and seamlessly integrating with RoPE, TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation of language modeling tasks, we demonstrate that T6 exceeds the performance of standard Transformer baselines including MHA, MQA, GQA, and MLA across various metrics, including perplexity and a range of renowned evaluation benchmarks. Notably, TPA's memory efficiency enables the processing of significantly longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. The code is available at https://github.com/tensorgi/T6.",
      categories: ["cs.CL", "cs", "cs.LG", "cs.AI"],
      update_date: "2025-02-07T00:00:00",
      title: "Tensor Product Attention Is All You Need",
      arxiv_id: "2501.06425",
      authors: [
        "Liu Yifeng",
        "Yao Andrew Chi-Chih",
        "Yifeng Liu",
        "Yuan Yang",
        "Gu Quanquan",
        "Zhang Yifan",
        "Qin Zhen",
        "Yuan Huizhuo",
      ],
      journal_ref: null,
      comments: "31 pages, 6 figures",
      create_date: "2025-01-11T00:00:00",
    },
    {
      doi: null,
      submitter: "Cem Subakan",
      abstract:
        "Recurrent Neural Networks (RNNs) have long been the dominant architecture in sequence-to-sequence learning. RNNs, however, are inherently sequential models that do not allow parallelization of their computations. Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent computations with a multi-head attention mechanism. In this paper, we propose the SepFormer, a novel RNN-free Transformer-based neural network for speech separation. The SepFormer learns short and long-term dependencies with a multi-scale approach that employs transformers. The proposed model achieves state-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets. It reaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an SI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the parallelization advantages of Transformers and achieves a competitive performance even when downsampling the encoded representation by a factor of 8. It is thus significantly faster and it is less memory-demanding than the latest speech separation systems with comparable performance.",
      categories: ["eess.AS", "cs", "cs.LG", "eess.SP", "cs.SD", "eess"],
      update_date: "2021-03-08T00:00:00",
      title: "Attention is All You Need in Speech Separation",
      arxiv_id: "2010.13154",
      authors: [
        "Zhong Jianyuan",
        "Cornell Samuele",
        "Subakan Cem",
        "Bronzi Mirko",
        "Ravanelli Mirco",
        "Cem Subakan",
      ],
      journal_ref: null,
      comments: "Accepted to ICASSP 2021",
      create_date: "2020-10-25T00:00:00",
    },
    {
      doi: null,
      submitter: "Tassilo Klein",
      abstract:
        "The recently introduced BERT model exhibits strong performance on several language understanding benchmarks. In this paper, we describe a simple re-implementation of BERT for commonsense reasoning. We show that the attentions produced by BERT can be directly utilized for tasks such as the Pronoun Disambiguation Problem and Winograd Schema Challenge. Our proposed attention-guided commonsense reasoning method is conceptually simple yet empirically powerful. Experimental analysis on multiple datasets demonstrates that our proposed system performs remarkably well on all cases while outperforming the previously reported state of the art by a margin. While results suggest that BERT seems to implicitly learn to establish complex relationships between entities, solving commonsense reasoning tasks might require more than unsupervised models learned from huge text corpora.",
      categories: ["cs.CL", "cs", "cs.AI"],
      update_date: "2019-05-31T00:00:00",
      title: "Attention Is (not) All You Need for Commonsense Reasoning",
      arxiv_id: "1905.13497",
      authors: ["Klein Tassilo", "Nabi Moin", "Tassilo Klein"],
      journal_ref: null,
      comments: "to appear at ACL 2019",
      create_date: "2019-05-31T00:00:00",
    },
    {
      doi: null,
      submitter: "Sufeng Duan",
      abstract:
        "Taking greedy decoding algorithm as it should be, this work focuses on further strengthening the model itself for Chinese word segmentation (CWS), which results in an even more fast and more accurate CWS model. Our model consists of an attention only stacked encoder and a light enough decoder for the greedy segmentation plus two highway connections for smoother training, in which the encoder is composed of a newly proposed Transformer variant, Gaussian-masked Directional (GD) Transformer, and a biaffine attention scorer. With the effective encoder design, our model only needs to take unigram features for scoring. Our model is evaluated on SIGHAN Bakeoff benchmark datasets. The experimental results show that with the highest segmentation speed, the proposed model achieves new state-of-the-art or comparable performance against strong baselines in terms of strict closed test setting.",
      categories: ["cs.CL", "cs"],
      update_date: "2020-10-06T00:00:00",
      title: "Attention Is All You Need for Chinese Word Segmentation",
      arxiv_id: "1910.14537",
      authors: ["Duan Sufeng", "Sufeng Duan", "Zhao Hai"],
      journal_ref: null,
      comments: "11 pages, to appear in EMNLP 2020 as a long paper",
      create_date: "2019-10-31T00:00:00",
    },
    {
      doi: null,
      submitter: "Meng Fanxu",
      abstract:
        "Modern large language models (LLMs) often encounter communication bottlenecks on current hardware, rather than purely computational constraints. Multi-head Latent Attention (MLA) tackles this challenge by using low-rank matrices in the key-value (KV) layers, thereby allowing compressed latent KV states to be cached. This approach significantly reduces the KV cache size relative to traditional multi-head attention, leading to faster inference. Moreover, MLA employs an up-projection matrix to increase expressiveness, trading additional computation for reduced communication overhead. Although MLA has demonstrated efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers still rely on Group Query Attention (GQA) and have not announced any plans to adopt MLA. In this paper, we show that GQA can always be represented by MLA while maintaining the same KV cache overhead, but the converse does not hold. To encourage broader use of MLA, we introduce TransMLA, a post-training method that converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen, Mixtral) into MLA-based models. After conversion, the model can undergo additional training to boost expressiveness without increasing the KV cache size. Furthermore, we plan to develop MLA-specific inference acceleration techniques to preserve low latency in transformed models, thus enabling more efficient distillation of Deepseek R1.",
      categories: ["cs.AI", "cs", "cs.LG"],
      update_date: "2025-02-13T00:00:00",
      title: "TransMLA: Multi-Head Latent Attention Is All You Need",
      arxiv_id: "2502.07864",
      authors: ["Yao Zengwei", "Meng Fanxu", "Zhang Muhan"],
      journal_ref: null,
      comments: "https://github.com/fxmeng/TransMLA",
      create_date: "2025-02-11T00:00:00",
    },
    {
      doi: null,
      submitter: "Gumaro Rendon",
      abstract:
        "The work here enables linear cost-scaling with evolution time t while keeping polylog (1/œµ) scaling and no extra block-encoding qubits, where œµ is the algorithmic error. This is achieved through product formulas, stable interpolation (Chebyshev), and to calculate the needed fractional queries, cardinal sine interpolation is used.",
      categories: ["quant-ph"],
      update_date: "2025-02-19T00:00:00",
      title: "All you need is Trotter",
      arxiv_id: "2311.01533",
      authors: ["Rendon Gumaro", "Gumaro Rendon"],
      journal_ref: null,
      comments: "17 pages",
      create_date: "2023-11-02T00:00:00",
    },
    {
      doi: null,
      submitter: "Ning Xu",
      abstract:
        'In the fundamental statistics course, students are taught to remember the well-known saying: "Correlation is not Causation". Till now, statistics (i.e., correlation) have developed various successful frameworks, such as Transformer and Pre-training large-scale models, which have stacked multiple parallel self-attention blocks to imitate a wide range of tasks. However, in the causation community, how to build an integrated causal framework still remains an untouched domain despite its excellent intervention capabilities. In this paper, we propose the Causal Graph Routing (CGR) framework, an integrated causal scheme relying entirely on the intervention mechanisms to reveal the cause-effect forces hidden in data. Specifically, CGR is composed of a stack of causal layers. Each layer includes a set of parallel deconfounding blocks from different causal graphs. We combine these blocks via the concept of the proposed sufficient cause, which allows the model to dynamically select the suitable deconfounding methods in each layer. CGR is implemented as the stacked networks, integrating no confounder, back-door adjustment, front-door adjustment, and probability of sufficient cause. We evaluate this framework on two classical tasks of CV and NLP. Experiments show CGR can surpass the current state-of-the-art methods on both Visual Question Answer and Long Document Classification tasks. In particular, CGR has great potential in building the "causal" pre-training large-scale model that effectively generalizes to diverse tasks. It will improve the machines\' comprehension of causal relationships within a broader semantic space.',
      categories: ["cs.AI", "cs"],
      update_date: "2023-11-21T00:00:00",
      title: "Causality is all you need",
      arxiv_id: "2311.12307",
      authors: [
        "Tian Hongshuo",
        "Ning Xu",
        "Zhang Yongdong",
        "Gao Yifei",
        "Liu An-An",
        "Xu Ning",
      ],
      journal_ref: null,
      comments: "",
      create_date: "2023-11-21T00:00:00",
    },
    {
      doi: null,
      submitter: "Zehua Cheng",
      abstract:
        "Region proposal mechanisms are essential for existing deep learning approaches to object detection in images. Although they can generally achieve a good detection performance under normal circumstances, their recall in a scene with extreme cases is unacceptably low. This is mainly because bounding box annotations contain much environment noise information, and non-maximum suppression (NMS) is required to select target boxes. Therefore, in this paper, we propose the first anchor-free and NMS-free object detection model called weakly supervised multimodal annotation segmentation (WSMA-Seg), which utilizes segmentation models to achieve an accurate and robust object detection without NMS. In WSMA-Seg, multimodal annotations are proposed to achieve an instance-aware segmentation using weakly supervised bounding boxes; we also develop a run-data-based following algorithm to trace contours of objects. In addition, we propose a multi-scale pooling segmentation (MSP-Seg) as the underlying segmentation model of WSMA-Seg to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA-Seg. Experimental results on multiple datasets show that the proposed WSMA-Seg approach outperforms the state-of-the-art detectors.",
      categories: ["cs", "cs.CV"],
      update_date: "2019-05-26T00:00:00",
      title: "Segmentation is All You Need",
      arxiv_id: "1904.13300",
      authors: [
        "Wang Weiyang",
        "Cheng Zehua",
        "Zehua Cheng",
        "Wu Yuxiang",
        "Xu Zhenghua",
        "Lukasiewicz Thomas",
      ],
      journal_ref: null,
      comments: "10 Pages",
      create_date: "2019-04-30T00:00:00",
    },
    {
      doi: null,
      submitter: "Oded Naor",
      abstract:
        "We present DAG-Rider, the first asynchronous Byzantine Atomic Broadcast protocol that achieves optimal resilience, optimal amortized communication complexity, and optimal time complexity. DAG-Rider is post-quantum safe and ensures that all messages proposed by correct processes eventually get decided. We construct DAG-Rider in two layers: In the first layer, processes reliably broadcast their proposals and build a structured Directed Acyclic Graph (DAG) of the communication among them. In the second layer, processes locally observe their DAGs and totally order all proposals with no extra communication.",
      categories: ["cs.DC", "cs"],
      update_date: "2021-06-04T00:00:00",
      title: "All You Need is DAG",
      arxiv_id: "2102.08325",
      authors: [
        "Naor Oded",
        "Oded Naor",
        "Keidar Idit",
        "Kokoris-Kogias Eleftherios",
        "Spiegelman Alexander",
      ],
      journal_ref: null,
      comments: "",
      create_date: "2021-02-16T00:00:00",
    },
    {
      doi: null,
      submitter: "Qiming Chen",
      abstract:
        "The Convolution Neural Network (CNN) has demonstrated the unique advantage in audio, image and text learning; recently it has also challenged Recurrent Neural Networks (RNNs) with long short-term memory cells (LSTM) in sequence-to-sequence learning, since the computations involved in CNN are easily parallelizable whereas those involved in RNN are mostly sequential, leading to a performance bottleneck. However, unlike RNN, the native CNN lacks the history sensitivity required for sequence transformation; therefore enhancing the sequential order awareness, or position-sensitivity, becomes the key to make CNN the general deep learning model. In this work we introduce an extended CNN model with strengthen position-sensitivity, called PoseNet. A notable feature of PoseNet is the asymmetric treatment of position information in the encoder and the decoder. Experiments shows that PoseNet allows us to improve the accuracy of CNN based sequence-to-sequence learning significantly, achieving around 33-36 BLEU scores on the WMT 2014 English-to-German translation task, and around 44-46 BLEU scores on the English-to-French translation task.",
      categories: ["cs.CL", "cs", "cs.LG", "cs.NE"],
      update_date: "2017-12-27T00:00:00",
      title: "CNN Is All You Need",
      arxiv_id: "1712.09662",
      authors: ["Chen Qiming", "Qiming Chen", "Wu Ren"],
      journal_ref: null,
      comments: "",
      create_date: "2017-12-27T00:00:00",
    },
    {
      doi: null,
      submitter: "Frederic Prost",
      abstract:
        "In SPARQL, the query forms SELECT and CONSTRUCT have been the subject of several studies, both theoretical and practical. However, the composition of such queries and their interweaving when forming involved nested queries has not yet received much interest in the literature. We mainly tackle the problem of composing such queries. For this purpose, we introduce a language close to SPARQL where queries can be nested at will, involving either CONSTRUCT or SELECT query forms and provide a formal semantics for it. This semantics is based on a uniform interpretation of queries. This uniformity is due to an extension of the notion of RDF graphs to include isolated items such as variables. As a key feature of this work, we show how classical SELECT queries can be easily encoded as a particular case of CONSTRUCT queries.",
      categories: ["cs.DB", "cs", "cs.PL"],
      update_date: "2020-10-02T00:00:00",
      title: "All You Need Is CONSTRUCT",
      arxiv_id: "2010.00843",
      authors: [
        "Echahed Rachid",
        "Frederic Prost",
        "Prost Frederic",
        "Duval Dominique",
      ],
      journal_ref: null,
      comments: "",
      create_date: "2020-10-02T00:00:00",
    },
    {
      doi: null,
      submitter: "Maxim Ziatdinov",
      abstract:
        "We pose that microscopy offers an ideal real-world experimental environment for the development and deployment of active Bayesian and reinforcement learning methods. Indeed, the tremendous progress achieved by machine learning (ML) and artificial intelligence over the last decade has been largely achieved via the utilization of static data sets, from the paradigmatic MNIST to the bespoke corpora of text and image data used to train large models such as GPT3, DALLE and others. However, it is now recognized that continuous, minute improvements to state-of-the-art do not necessarily translate to advances in real-world applications. We argue that a promising pathway for the development of ML methods is via the route of domain-specific deployable algorithms in areas such as electron and scanning probe microscopy and chemical imaging. This will benefit both fundamental physical studies and serve as a test bed for more complex autonomous systems such as robotics and manufacturing. Favorable environment characteristics of scanning and electron microscopy include low risk, extensive availability of domain-specific priors and rewards, relatively small effects of exogeneous variables, and often the presence of both upstream first principles as well as downstream learnable physical models for both statics and dynamics. Recent developments in programmable interfaces, edge computing, and access to APIs facilitating microscope control, all render the deployment of ML codes on operational microscopes straightforward. We discuss these considerations and hope that these arguments will lead to creating a novel set of development targets for the ML community by accelerating both real-world ML applications and scientific progress.",
      categories: ["cond-mat.dis-nn", "cs", "cond-mat", "cs.LG"],
      update_date: "2022-10-12T00:00:00",
      title: "Microscopy is All You Need",
      arxiv_id: "2210.06526",
      authors: [
        "Maxim Ziatdinov",
        "Liu Yongtao",
        "Vasudevan Rama",
        "Kalinin Sergei V.",
        "Ghosh Ayana",
        "Roccapriore Kevin",
        "Ziatdinov Maxim",
      ],
      journal_ref: null,
      comments: "",
      create_date: "2022-10-12T00:00:00",
    },
    {
      doi: null,
      submitter: "Sara Atito",
      abstract:
        "Vision transformers have generated significant interest in the computer vision community because of their flexibility in exploiting contextual information, whether it is sharply confined local, or long range global. However, they are known to be data hungry. This has motivated the research in self-supervised transformer pretraining, which does not need to decode the semantic information conveyed by labels to link it to the image properties, but rather focuses directly on extracting a concise representation of the image data that reflects the notion of similarity, and is invariant to nuisance factors. The key vehicle for the self-learning process used by the majority of self-learning methods is the generation of multiple views of the training data and the creation of pretext tasks which use these views to define the notion of image similarity, and data integrity. However, this approach lacks the natural propensity to extract contextual information. We propose group masked model learning (GMML), a self-supervised learning (SSL) mechanism for pretraining vision transformers with the ability to extract the contextual information present in all the concepts in an image. GMML achieves this by manipulating randomly groups of connected tokens, ensuingly covering a meaningful part of a semantic concept, and then recovering the hidden semantic information from the visible part of the concept. GMML implicitly introduces a novel data augmentation process. Unlike most of the existing SSL approaches, GMML does not require momentum encoder, nor rely on careful implementation details such as large batches and gradient stopping, which are all artefacts of most of the current self-supervised learning techniques. The source code is publicly available for the community to train on bigger corpora: https://github.com/Sara-Ahmed/GMML.",
      categories: ["cs", "cs.CV"],
      update_date: "2022-05-30T00:00:00",
      title: "GMML is All you Need",
      arxiv_id: "2205.14986",
      authors: ["Awais Muhammad", "Atito Sara", "Kittler Josef", "Sara Atito"],
      journal_ref: null,
      comments: "",
      create_date: "2022-05-30T00:00:00",
    },
    {
      doi: "10.1145/3583780.3615497",
      submitter: "Mirza Mohtashim Alam",
      abstract:
        'Skilled employees are the most important pillars of an organization. Despite this, most organizations face high attrition and turnover rates. While several machine learning models have been developed to analyze attrition and its causal factors, the interpretations of those models remain opaque. In this paper, we propose the HR-DSS approach, which stands for Human Resource (HR) Decision Support System, and uses explainable AI for employee attrition problems. The system is designed to assist HR departments in interpreting the predictions provided by machine learning models. In our experiments, we employ eight machine learning models to provide predictions. We further process the results achieved by the best-performing model by the SHAP explainability process and use the SHAP values to generate natural language explanations which can be valuable for HR. Furthermore, using "What-if-analysis", we aim to observe plausible causes for attrition of an individual employee. The results show that by adjusting the specific dominant features of each individual, employee attrition can turn into employee retention through informative business decisions.',
      categories: ["cs.AI", "cs"],
      update_date: "2023-08-26T00:00:00",
      title: "Retention Is All You Need",
      arxiv_id: "2304.03103",
      authors: [
        "Alam Mirza Ariful",
        "Martin Michael",
        "Mirza Mohtashim Alam",
        "Lehmann Jens",
        "Alam Mirza Mohtashim",
        "Mohiuddin Karishma",
        "Welke Pascal",
        "Vahdati Sahar",
      ],
      journal_ref: null,
      comments: "Accepted at CIKM 2023 Applied Research Track",
      create_date: "2023-04-06T00:00:00",
    },
    {
      doi: null,
      submitter: "Tian Lan",
      abstract:
        "The dominant text generation models compose the output by sequentially selecting words from a fixed vocabulary. In this paper, we formulate text generation as progressively copying text segments (e.g., words or phrases) from an existing text collection. We compute the contextualized representations of meaningful text segments and index them using efficient vector search toolkits. The task of text generation is then decomposed into a series of copy-and-paste operations: at each time step, we seek suitable text spans from the text collection rather than selecting from a standalone vocabulary. Experiments on the standard language modeling benchmark (WikiText-103) show that our approach achieves better generation quality according to both automatic and human evaluations. Besides, its inference efficiency is comparable to token-level autoregressive models thanks to the reduction of decoding steps. We also show that our approach allows for effective domain adaptation by simply switching to domain-specific text collection without extra training. Finally, we observe that our approach attains additional performance gains by simply scaling up to larger text collections, again without further training.[Our source codes are publicly available at <https://github.com/gmftbyGMFTBY/Copyisallyouneed>.]",
      categories: ["cs.CL", "cs", "cs.AI"],
      update_date: "2023-07-13T00:00:00",
      title: "Copy Is All You Need",
      arxiv_id: "2307.06962",
      authors: [
        "Wang Yan",
        "Lan Tian",
        "Cai Deng",
        "Tian Lan",
        "Mao Xian-Ling",
        "Huang Heyan",
      ],
      journal_ref: null,
      comments: "",
      create_date: "2023-07-13T00:00:00",
    },
    {
      doi: null,
      submitter: "Hendrik Kempt",
      abstract:
        'The strive to make AI applications "safe" has led to the development of safety-measures as the main or even sole normative requirement of their permissible use. Similar can be attested to the latest version of chatbots, such as chatGPT. In this view, if they are "safe", they are supposed to be permissible to deploy. This approach, which we call "safety-normativity", is rather limited in solving the emerging issues that chatGPT and other chatbots have caused thus far. In answering this limitation, in this paper we argue for limiting chatbots in the range of topics they can chat about according to the normative concept of appropriateness. We argue that rather than looking for "safety" in a chatbot\'s utterances to determine what they may and may not say, we ought to assess those utterances according to three forms of appropriateness: technical-discursive, social, and moral. We then spell out what requirements for chatbots follow from these forms of appropriateness to avoid the limits of previous accounts: positionality, acceptability, and value alignment (PAVA). With these in mind, we may be able to determine what a chatbot may and may not say. Lastly, one initial suggestion is to use challenge sets, specifically designed for appropriateness, as a validation method.',
      categories: ["cs.AI", "cs", "cs.CL", "cs.HC"],
      update_date: "2023-04-27T00:00:00",
      title: "Appropriateness is all you need!",
      arxiv_id: "2304.14553",
      authors: [
        "Hendrik Kempt",
        "Lavie Alon",
        "Kempt Hendrik",
        "Nagel Saskia K.",
      ],
      journal_ref: null,
      comments: "",
      create_date: "2023-04-27T00:00:00",
    },
    {
      doi: null,
      submitter: "Khubaib Ahmed",
      abstract:
        "Going beyond 'dendritic democracy', we introduce a 'democracy of local processors', termed Cooperator. Here we compare their capabilities when used in permutation-invariant neural networks for reinforcement learning (RL), with machine learning algorithms based on Transformers, such as ChatGPT. Transformers are based on the long-standing conception of integrate-and-fire 'point' neurons, whereas Cooperator is inspired by recent neurobiological breakthroughs suggesting that the cellular foundations of mental life depend on context-sensitive pyramidal neurons in the neocortex which have two functionally distinct points. We show that when used for RL, an algorithm based on Cooperator learns far quicker than that based on Transformer, even while having the same number of parameters.",
      categories: ["cs.AI", "cs", "cs.LG", "cs.NE"],
      update_date: "2023-05-16T00:00:00",
      title: "Cooperation Is All You Need",
      arxiv_id: "2305.10449",
      authors: [
        "Muzaffar Junaid",
        "Ahmed Khubaib",
        "Raza Mohsin",
        "Khubaib Ahmed",
        "Adeel Ahsan",
      ],
      journal_ref: null,
      comments: "",
      create_date: "2023-05-16T00:00:00",
    },
    {
      doi: null,
      submitter: "Mingze Ni",
      abstract:
        'In classification tasks, achieving a harmonious balance between exploration and precision is of paramount importance. To this end, this research introduces two novel deep learning models, SleepNet and DreamNet, to strike this balance. SleepNet seamlessly integrates supervised learning with unsupervised ‚Äúsleep" stages using pre-trained encoder models. Dedicated neurons within SleepNet are embedded in these unsupervised features, forming intermittent ‚Äúsleep" blocks that facilitate exploratory learning. Building upon the foundation of SleepNet, DreamNet employs full encoder-decoder frameworks to reconstruct the hidden states, mimicking the human "dreaming" process. This reconstruction process enables further exploration and refinement of the learned representations. Moreover, the principle ideas of our SleepNet and DreamNet are generic and can be applied to both computer vision and natural language processing downstream tasks. Through extensive empirical evaluations on diverse image and text datasets, SleepNet and DreanNet have demonstrated superior performance compared to state-of-the-art models, showcasing the strengths of unsupervised exploration and supervised precision afforded by our innovative approaches.',
      categories: ["cs.AI", "cs", "cs.LG", "cs.CV"],
      update_date: "2024-09-15T00:00:00",
      title: "Dreaming is All You Need",
      arxiv_id: "2409.01633",
      authors: ["Mingze Ni", "Liu Wei", "Ni Mingze"],
      journal_ref: null,
      comments: "",
      create_date: "2024-09-03T00:00:00",
    },
    {
      doi: null,
      submitter: "Xin Li",
      abstract:
        "One can drive safely with a GPS without memorizing a world map (not to mention the dark regions that humans have never explored). Such a locality-based attention mechanism has a profound implication on our understanding of how the brain works. This paper refines the existing embodied cognition framework by turning the locality from a constraint to an advantage. Analogous to GPS-based navigation, positioning represents a computationally more efficient solution to flexible behaviors than reconstruction. This simple intuition implies that positioning is all you need to understand cognitive functions generated by hippocampal-neocortical systems. That is, the neocortex generates thousands of local maps whose indexing is maintained by the hippocampus. Geometrically, we present a simple manifold positioning framework to explain the principle of localized embodied cognition. The positioning operation implemented by the attention mechanism can be interpreted as a nonlinear projection linking the discovery of local subspace structure by the neocortex (a sensorimotor machine interacting with the world locally) for the navigation task in mind without discovering global manifold topology.",
      categories: ["nlin", "q-bio", "nlin.AO", "q-bio.NC"],
      update_date: "2024-04-03T00:00:00",
      title: "Positioning is All You Need",
      arxiv_id: "2404.01183",
      authors: ["Xin Li", "Li Xin"],
      journal_ref: null,
      comments: "",
      create_date: "2024-04-01T00:00:00",
    },
    {
      doi: null,
      submitter: "Lennart Schneider",
      abstract:
        "Neural architecture search (NAS) promises to make deep learning accessible to non-experts by automating architecture engineering of deep neural networks. BANANAS is one state-of-the-art NAS method that is embedded within the Bayesian optimization framework. Recent experimental findings have demonstrated the strong performance of BANANAS on the NAS-Bench-101 benchmark being determined by its path encoding and not its choice of surrogate model. We present experimental results suggesting that the performance of BANANAS on the NAS-Bench-301 benchmark is determined by its acquisition function optimizer, which minimally mutates the incumbent.",
      categories: ["cs.NE", "cs", "cs.LG"],
      update_date: "2021-07-04T00:00:00",
      title: "Mutation is all you need",
      arxiv_id: "2107.07343",
      authors: [
        "Pfisterer Florian",
        "Schneider Lennart",
        "Binder Martin",
        "Lennart Schneider",
        "Bischl Bernd",
      ],
      journal_ref: null,
      comments:
        "Accepted for the 8th ICML Workshop on Automated Machine Learning (2021). 10 pages, 1 table, 3 figures",
      create_date: "2021-07-04T00:00:00",
    },
    {
      doi: null,
      submitter: "Konstantin Riedl",
      abstract:
        "In this paper we provide a novel analytical perspective on the theoretical understanding of gradient-based learning algorithms by interpreting consensus-based optimization (CBO), a recently proposed multi-particle derivative-free optimization method, as a stochastic relaxation of gradient descent. Remarkably, we observe that through communication of the particles, CBO exhibits a stochastic gradient descent (SGD)-like behavior despite solely relying on evaluations of the objective function. The fundamental value of such link between CBO and SGD lies in the fact that CBO is provably globally convergent to global minimizers for ample classes of nonsmooth and nonconvex objective functions, hence, on the one side, offering a novel explanation for the success of stochastic relaxations of gradient descent. On the other side, contrary to the conventional wisdom for which zero-order methods ought to be inefficient or not to possess generalization abilities, our results unveil an intrinsic gradient descent nature of such heuristics. This viewpoint furthermore complements previous insights into the working principles of CBO, which describe the dynamics in the mean-field limit through a nonlinear nonlocal partial differential equation that allows to alleviate complexities of the nonconvex function landscape. Our proofs leverage a completely nonsmooth analysis, which combines a novel quantitative version of the Laplace principle (log-sum-exp trick) and the minimizing movement scheme (proximal iteration). In doing so, we furnish useful and precise insights that explain how stochastic perturbations of gradient descent overcome energy barriers and reach deep levels of nonconvex functions. Instructive numerical illustrations support the provided theoretical insights.",
      categories: [
        "math",
        "stat",
        "math.NA",
        "math.OC",
        "stat.ML",
        "cs",
        "cs.LG",
        "cs.NA",
      ],
      update_date: "2023-06-16T00:00:00",
      title: "Gradient is All You Need?",
      arxiv_id: "2306.09778",
      authors: [
        "Klock Timo",
        "Fornasier Massimo",
        "Geldhauser Carina",
        "Riedl Konstantin",
        "Konstantin Riedl",
      ],
      journal_ref: null,
      comments: "38 pages, 4 figures",
      create_date: "2023-06-16T00:00:00",
    },
    {
      doi: null,
      submitter: "Burak Kilic",
      abstract:
        "In this study, we analyze data-scarce classification scenarios, where available labeled legal data is small and imbalanced, potentially hurting the quality of the results. We focused on two finetuning objectives; SetFit (Sentence Transformer Finetuning), a contrastive learning setup, and a vanilla finetuning setup on a legal provision classification task. Additionally, we compare the features that are extracted with LIME (Local Interpretable Model-agnostic Explanations) to see which particular features contributed to the model's classification decisions. The results show that a contrastive setup with SetFit performed better than vanilla finetuning while using a fraction of the training samples. LIME results show that the contrastive learning approach helps boost both positive and negative features which are legally informative and contribute to the classification results. Thus a model finetuned with a contrastive objective seems to base its decisions more confidently on legally informative features.",
      categories: ["cs.CL", "cs", "cs.AI"],
      update_date: "2023-07-06T00:00:00",
      title: "Contrast Is All You Need",
      arxiv_id: "2307.02882",
      authors: ["Gatt Albert", "Kilic Burak", "Burak Kilic", "Bex Florix"],
      journal_ref: null,
      comments: "10 pages + bib, 12 figures, ACAIL2023/ASAIL2023 Workshop",
      create_date: "2023-07-06T00:00:00",
    },
    {
      doi: null,
      submitter: "Paul Kronlund-Drouault",
      abstract:
        "As Machine Learning (ML) is still a recent field of study, especially outside the realm of abstract Mathematics and Computer Science, few works have been conducted on the political aspect of large Language Models (LLMs), and more particularly about the alignment process and its political dimension. This process can be as simple as prompt engineering but is also very complex and can affect completely unrelated notions. For example, politically directed alignment has a very strong impact on an LLM's embedding space and the relative position of political notions in such a space. Using special tools to evaluate general political bias and analyze the effects of alignment, we can gather new data to understand its causes and possible consequences on society. Indeed, by taking a socio-political approach, we can hypothesize that most big LLMs are aligned with what Marxist philosophy calls the 'dominant ideology.' As AI's role in political decision-making, at the citizen's scale but also in government agencies, such biases can have huge effects on societal change, either by creating new and insidious pathways for societal uniformity or by allowing disguised extremist views to gain traction among the people.",
      categories: ["cs.AI", "cs", "cs.CY"],
      update_date: "2025-04-01T00:00:00",
      title: "Propaganda is all you need",
      arxiv_id: "2410.01810",
      authors: ["Paul Kronlund-Drouault", "Kronlund-Drouault Paul"],
      journal_ref: null,
      comments: "",
      create_date: "2024-09-13T00:00:00",
    },
    {
      doi: null,
      submitter: "Tao Hu",
      abstract:
        "In generative models, two paradigms have gained attraction in various applications: next-set prediction-based Masked Generative Models and next-noise prediction-based Non-Autoregressive Models, e.g., Diffusion Models. In this work, we propose using discrete-state models to connect them and explore their scalability in the vision domain. First, we conduct a step-by-step analysis in a unified design space across two types of models including timestep-independence, noise schedule, temperature, guidance strength, etc in a scalable manner. Second, we re-cast typical discriminative tasks, e.g., image segmentation, as an unmasking process from [MASK] tokens on a discrete-state model. This enables us to perform various sampling processes, including flexible conditional sampling by only training once to model the joint distribution. All aforementioned explorations lead to our framework named Discrete Interpolants, which enables us to achieve state-of-the-art or competitive performance compared to previous discrete-state based methods in various benchmarks, like ImageNet256, MS COCO, and video dataset FaceForensics. In summary, by leveraging [MASK] in discrete-state models, we can bridge Masked Generative and Non-autoregressive Diffusion models, as well as generative and discriminative tasks.",
      categories: ["cs.AI", "cs", "cs.CV"],
      update_date: "2024-12-10T00:00:00",
      title: "[MASK] is All You Need",
      arxiv_id: "2412.06787",
      authors: ["Tao Hu", "Ommer Bj√∂rn", "Hu Vincent Tao"],
      journal_ref: null,
      comments:
        "Technical Report (WIP), Project Page(code, model, dataset): https://compvis.github.io/mask/",
      create_date: "2024-12-09T00:00:00",
    },
  ],
  available_facets: [
    {
      field: "categories",
      value: "cs",
      count: 624172,
    },
    {
      field: "categories",
      value: "math",
      count: 518435,
    },
    {
      field: "categories",
      value: "cond-mat",
      count: 310746,
    },
    {
      field: "categories",
      value: "astro-ph",
      count: 300063,
    },
    {
      field: "categories",
      value: "physics",
      count: 224252,
    },
    {
      field: "categories",
      value: "cs.LG",
      count: 177665,
    },
    {
      field: "categories",
      value: "hep-ph",
      count: 134215,
    },
    {
      field: "categories",
      value: "hep-th",
      count: 129417,
    },
    {
      field: "categories",
      value: "cs.CV",
      count: 128960,
    },
    {
      field: "categories",
      value: "quant-ph",
      count: 128024,
    },
    {
      field: "authors",
      value: "EPTCS",
      count: 3089,
    },
    {
      field: "authors",
      value: "Zhang Y.",
      count: 2140,
    },
    {
      field: "authors",
      value: "Liu Yang",
      count: 1806,
    },
    {
      field: "authors",
      value: "Wang Y.",
      count: 1621,
    },
    {
      field: "authors",
      value: "Wang Wei",
      count: 1481,
    },
    {
      field: "authors",
      value: "Li Y.",
      count: 1387,
    },
    {
      field: "authors",
      value: "Zhang Lei",
      count: 1385,
    },
    {
      field: "authors",
      value: "Wang Z.",
      count: 1281,
    },
    {
      field: "authors",
      value: "Taniguchi Takashi",
      count: 1277,
    },
    {
      field: "authors",
      value: "Zhang L.",
      count: 1269,
    },
  ],
  found_per_year: {
    "1986": 0,
    "1987": 0,
    "1988": 0,
    "1989": 0,
    "1990": 11,
    "1991": 148,
    "1992": 1460,
    "1993": 3393,
    "1994": 5573,
    "1995": 7872,
    "1996": 10083,
    "1997": 13223,
    "1998": 17255,
    "1999": 20165,
    "2000": 22654,
    "2001": 24514,
    "2002": 26441,
    "2003": 29162,
    "2004": 32347,
    "2005": 35007,
    "2006": 37350,
    "2007": 41476,
    "2008": 43919,
    "2009": 48086,
    "2010": 52752,
    "2011": 57330,
    "2012": 64088,
    "2013": 70595,
    "2014": 75065,
    "2015": 81063,
    "2016": 88763,
    "2017": 96252,
    "2018": 109508,
    "2019": 122251,
    "2020": 141911,
    "2021": 150255,
    "2022": 153138,
    "2023": 170348,
    "2024": 217899,
    "2025": 74841,
  },
};
